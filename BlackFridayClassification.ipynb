{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlackFridayClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eman-1111/BlackFridayAnalytics/blob/master/BlackFridayClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nASapAYrf5F-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Read Data\n",
        "\n",
        "First, we read the csv file as a Pnadas dataframe. The dataset is available on Kaggle\n"
      ]
    },
    {
      "metadata": {
        "id": "qeygLQs5gE8I",
        "colab_type": "code",
        "outputId": "e97c7838-58e6-404c-e742-12c2c8733f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "32uTIeEcKstf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "h2EuQ4Nhf5GI",
        "colab_type": "code",
        "outputId": "a5d53f0e-a3c8-4ee3-e317-7066e18262c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "black_friday_data = pd.read_csv(\"./drive/My Drive/BlackFriday.csv\")\n",
        "black_friday_data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Product_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00069042</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00248942</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00087842</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00085442</td>\n",
              "      <td>F</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000002</td>\n",
              "      <td>P00285442</td>\n",
              "      <td>M</td>\n",
              "      <td>55+</td>\n",
              "      <td>16</td>\n",
              "      <td>C</td>\n",
              "      <td>4+</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
              "0  1000001  P00069042      F  0-17          10             A   \n",
              "1  1000001  P00248942      F  0-17          10             A   \n",
              "2  1000001  P00087842      F  0-17          10             A   \n",
              "3  1000001  P00085442      F  0-17          10             A   \n",
              "4  1000002  P00285442      M   55+          16             C   \n",
              "\n",
              "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
              "0                          2               0                   3   \n",
              "1                          2               0                   1   \n",
              "2                          2               0                  12   \n",
              "3                          2               0                  12   \n",
              "4                         4+               0                   8   \n",
              "\n",
              "   Product_Category_2  Product_Category_3  Purchase  \n",
              "0                 NaN                 NaN      8370  \n",
              "1                 6.0                14.0     15200  \n",
              "2                 NaN                 NaN      1422  \n",
              "3                14.0                 NaN      1057  \n",
              "4                 NaN                 NaN      7969  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "HKz4nl82f5Gb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are some missing values. Let's find out which columns have missing values and how many of them are there"
      ]
    },
    {
      "metadata": {
        "id": "BSl-vCtjf5Gh",
        "colab_type": "code",
        "outputId": "6bc4df03-c02f-4acf-aae1-7d37bd753aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "for column_name in black_friday_data.columns:\n",
        "    missing_value = black_friday_data[column_name].isnull().sum()\n",
        "    print(column_name, missing_value)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User_ID 0\n",
            "Product_ID 0\n",
            "Gender 0\n",
            "Age 0\n",
            "Occupation 0\n",
            "City_Category 0\n",
            "Stay_In_Current_City_Years 0\n",
            "Marital_Status 0\n",
            "Product_Category_1 0\n",
            "Product_Category_2 166986\n",
            "Product_Category_3 373299\n",
            "Purchase 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WOUdh_lUf5Gu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will have to take this in consideration in our feature engineering phase. If we intend to keep these columns however, then we need to replace the missing values by something our **deep nerual network can recognize**. Since both columns represent categorical data, we can create a new category that represent missing values.\n",
        "\n",
        "As you can see these columns represent their categories using integer values. Let's examine how many categories are there"
      ]
    },
    {
      "metadata": {
        "id": "skVNEHKPf5Gx",
        "colab_type": "code",
        "outputId": "f23912d0-7431-432d-d092-d63b658bfb15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "unique_categories = []\n",
        "for col_name in ['Product_Category_1', 'Product_Category_2', 'Product_Category_3']:\n",
        "    unique_categories += list(black_friday_data[col_name].unique())\n",
        "set(unique_categories)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{nan, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, nan}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "hN2LSWM_f5G7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like the categories start at 1, so we can use 0 to represent missing information"
      ]
    },
    {
      "metadata": {
        "id": "zFbwrzpFf5G-",
        "colab_type": "code",
        "outputId": "3ca7b6e7-b306-4f2e-b40b-39fca03cb45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        "black_friday_data['Product_Category_2'].fillna(0, inplace=True)\n",
        "black_friday_data['Product_Category_3'].fillna(0, inplace=True)\n",
        "black_friday_data['Gender'].replace(('M', 'F'), (1, 0), inplace=True)\n",
        "\n",
        "black_friday_data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Product_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00069042</td>\n",
              "      <td>0</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00248942</td>\n",
              "      <td>0</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00087842</td>\n",
              "      <td>0</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000001</td>\n",
              "      <td>P00085442</td>\n",
              "      <td>0</td>\n",
              "      <td>0-17</td>\n",
              "      <td>10</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000002</td>\n",
              "      <td>P00285442</td>\n",
              "      <td>1</td>\n",
              "      <td>55+</td>\n",
              "      <td>16</td>\n",
              "      <td>C</td>\n",
              "      <td>4+</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   User_ID Product_ID  Gender   Age  Occupation City_Category  \\\n",
              "0  1000001  P00069042       0  0-17          10             A   \n",
              "1  1000001  P00248942       0  0-17          10             A   \n",
              "2  1000001  P00087842       0  0-17          10             A   \n",
              "3  1000001  P00085442       0  0-17          10             A   \n",
              "4  1000002  P00285442       1   55+          16             C   \n",
              "\n",
              "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
              "0                          2               0                   3   \n",
              "1                          2               0                   1   \n",
              "2                          2               0                  12   \n",
              "3                          2               0                  12   \n",
              "4                         4+               0                   8   \n",
              "\n",
              "   Product_Category_2  Product_Category_3  Purchase  \n",
              "0                 0.0                 0.0      8370  \n",
              "1                 6.0                14.0     15200  \n",
              "2                 0.0                 0.0      1422  \n",
              "3                14.0                 0.0      1057  \n",
              "4                 0.0                 0.0      7969  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "o_YlUOROf5HH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # black_friday_data = black_friday_data.groupby(['User_ID','Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status','Occupation'])['Product_Category_1'].agg('max')\n",
        "# black_friday_data = black_friday_data.groupby(['User_ID','Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status','Occupation','Product_Category_1']).agg({'Purchase':'sum'})\n",
        "# # black_friday_data = black_friday_data.to_frame().reset_index()\n",
        "# black_friday_data = black_friday_data.add_suffix('').reset_index()\n",
        "# black_friday_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1O50Ctof5HU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "see"
      ]
    },
    {
      "metadata": {
        "id": "_kkMI7ZSf5HZ",
        "colab_type": "code",
        "outputId": "38e2afc9-90b4-4f55-d074-542beaf6c7e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "black_friday_data['Marital_Status'].value_counts().plot(kind='barh', title='Marital_Status')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f06c5d54f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAEHCAYAAABshbdkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGWlJKsxIS1JhRlqSCvs/Ni85HvNEZ2UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f06c5c98588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fwS_mnBtf5Ht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# black_friday_data.drop(black_friday_data.query('Marital_Status == 0').sample(frac=.7).index)\n",
        "black_friday_data = black_friday_data.drop(black_friday_data.query('Marital_Status == 0').sample(frac=0.3).index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqEOYV5Cf5H2",
        "colab_type": "code",
        "outputId": "2676f7b6-4f66-44d9-ccfa-ed2ee27bec04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "black_friday_data['Marital_Status'].value_counts().plot(kind='barh', title='Marital_Status')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f06c5c98e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAEHCAYAAABshbdkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f06c5c9b7f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "cWNFD8FUf5H-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep neural network \n"
      ]
    },
    {
      "metadata": {
        "id": "lTQaz_8Xf5IA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### one-hot encoding\n",
        "\n",
        "use one-hot encoding on categorical data so it would be eaiser for nerual network  to find correlation, nerual network  require numerical input and output variables"
      ]
    },
    {
      "metadata": {
        "id": "MJjdBRFmf5IC",
        "colab_type": "code",
        "outputId": "0e69872f-d9a8-419d-e800-d2de33115aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "data = black_friday_data\n",
        "\n",
        "dummy_fields = [ 'City_Category','Stay_In_Current_City_Years','Age']\n",
        "for each in dummy_fields:\n",
        "    dummies = pd.get_dummies(data[each], prefix=each, drop_first=False)\n",
        "    data = pd.concat([data, dummies], axis=1)\n",
        "\n",
        "fields_to_drop = ['User_ID','Product_ID','City_Category','Stay_In_Current_City_Years','Age']\n",
        "data = data.drop(fields_to_drop, axis=1)\n",
        "data.head()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "      <th>City_Category_A</th>\n",
              "      <th>City_Category_B</th>\n",
              "      <th>City_Category_C</th>\n",
              "      <th>...</th>\n",
              "      <th>Stay_In_Current_City_Years_2</th>\n",
              "      <th>Stay_In_Current_City_Years_3</th>\n",
              "      <th>Stay_In_Current_City_Years_4+</th>\n",
              "      <th>Age_0-17</th>\n",
              "      <th>Age_18-25</th>\n",
              "      <th>Age_26-35</th>\n",
              "      <th>Age_36-45</th>\n",
              "      <th>Age_46-50</th>\n",
              "      <th>Age_51-55</th>\n",
              "      <th>Age_55+</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8370</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15200</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1057</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15227</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>19215</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender  Occupation  Marital_Status  Product_Category_1  Product_Category_2  \\\n",
              "0       0          10               0                   3                 0.0   \n",
              "1       0          10               0                   1                 6.0   \n",
              "3       0          10               0                  12                14.0   \n",
              "5       1          15               0                   1                 2.0   \n",
              "6       1           7               1                   1                 8.0   \n",
              "\n",
              "   Product_Category_3  Purchase  City_Category_A  City_Category_B  \\\n",
              "0                 0.0      8370                1                0   \n",
              "1                14.0     15200                1                0   \n",
              "3                 0.0      1057                1                0   \n",
              "5                 0.0     15227                1                0   \n",
              "6                17.0     19215                0                1   \n",
              "\n",
              "   City_Category_C   ...     Stay_In_Current_City_Years_2  \\\n",
              "0                0   ...                                1   \n",
              "1                0   ...                                1   \n",
              "3                0   ...                                1   \n",
              "5                0   ...                                0   \n",
              "6                0   ...                                1   \n",
              "\n",
              "   Stay_In_Current_City_Years_3  Stay_In_Current_City_Years_4+  Age_0-17  \\\n",
              "0                             0                              0         1   \n",
              "1                             0                              0         1   \n",
              "3                             0                              0         1   \n",
              "5                             1                              0         0   \n",
              "6                             0                              0         0   \n",
              "\n",
              "   Age_18-25  Age_26-35  Age_36-45  Age_46-50  Age_51-55  Age_55+  \n",
              "0          0          0          0          0          0        0  \n",
              "1          0          0          0          0          0        0  \n",
              "3          0          0          0          0          0        0  \n",
              "5          0          1          0          0          0        0  \n",
              "6          0          0          0          1          0        0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "cIodaNBOf5IP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize Data\n",
        "Reduce differences in the ranges of values,to make training faster .\n",
        "\n",
        "For **Purchase** standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.\n",
        "\n",
        "For the rest, we divide on the max number so the value be between 0 to 1"
      ]
    },
    {
      "metadata": {
        "id": "K2pbTrxXf5IR",
        "colab_type": "code",
        "outputId": "b7be74ec-d370-457e-a0c3-075819f58b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "mean, std = data[\"Purchase\"].mean(), data[\"Purchase\"].std()\n",
        "data.loc[:, \"Purchase\"] = (data[\"Purchase\"] - mean)/std\n",
        "\n",
        "data[\"Product_Category_1\"] = data[\"Product_Category_1\"] / 18\n",
        "data[\"Product_Category_2\"] = data[\"Product_Category_2\"] / 18\n",
        "data[\"Product_Category_3\"] = data[\"Product_Category_3\"] / 18\n",
        "data[\"Occupation\"] = data[\"Occupation\"] / 20\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "      <th>City_Category_A</th>\n",
              "      <th>City_Category_B</th>\n",
              "      <th>City_Category_C</th>\n",
              "      <th>...</th>\n",
              "      <th>Stay_In_Current_City_Years_2</th>\n",
              "      <th>Stay_In_Current_City_Years_3</th>\n",
              "      <th>Stay_In_Current_City_Years_4+</th>\n",
              "      <th>Age_0-17</th>\n",
              "      <th>Age_18-25</th>\n",
              "      <th>Age_26-35</th>\n",
              "      <th>Age_36-45</th>\n",
              "      <th>Age_46-50</th>\n",
              "      <th>Age_51-55</th>\n",
              "      <th>Age_55+</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.194365</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>1.177655</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.663411</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.183079</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>1.984194</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender  Occupation  Marital_Status  Product_Category_1  Product_Category_2  \\\n",
              "0       0        0.50               0            0.166667            0.000000   \n",
              "1       0        0.50               0            0.055556            0.333333   \n",
              "3       0        0.50               0            0.666667            0.777778   \n",
              "5       1        0.75               0            0.055556            0.111111   \n",
              "6       1        0.35               1            0.055556            0.444444   \n",
              "\n",
              "   Product_Category_3  Purchase  City_Category_A  City_Category_B  \\\n",
              "0            0.000000 -0.194365                1                0   \n",
              "1            0.777778  1.177655                1                0   \n",
              "3            0.000000 -1.663411                1                0   \n",
              "5            0.000000  1.183079                1                0   \n",
              "6            0.944444  1.984194                0                1   \n",
              "\n",
              "   City_Category_C   ...     Stay_In_Current_City_Years_2  \\\n",
              "0                0   ...                                1   \n",
              "1                0   ...                                1   \n",
              "3                0   ...                                1   \n",
              "5                0   ...                                0   \n",
              "6                0   ...                                1   \n",
              "\n",
              "   Stay_In_Current_City_Years_3  Stay_In_Current_City_Years_4+  Age_0-17  \\\n",
              "0                             0                              0         1   \n",
              "1                             0                              0         1   \n",
              "3                             0                              0         1   \n",
              "5                             1                              0         0   \n",
              "6                             0                              0         0   \n",
              "\n",
              "   Age_18-25  Age_26-35  Age_36-45  Age_46-50  Age_51-55  Age_55+  \n",
              "0          0          0          0          0          0        0  \n",
              "1          0          0          0          0          0        0  \n",
              "3          0          0          0          0          0        0  \n",
              "5          0          1          0          0          0        0  \n",
              "6          0          0          0          1          0        0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "YmTz1xX-f5Ib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting the data into training, testing, and validation sets\n",
        "\n",
        "We'll save approximately 40% data as validation and test set after we've trained the network. We'll use this set to make predictions and compare them with the actual numbers.\n",
        "\n",
        "Then Separate the data into features and targets.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VBBTWkP2f5Ie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "train, validation = train_test_split(train, test_size=0.2)\n",
        "\n",
        "\n",
        "target_fields = ['Marital_Status']\n",
        "\n",
        "train_features, train_targets = train.drop(target_fields, axis=1), train[target_fields]\n",
        "validation_features, validation_targets = validation.drop(target_fields, axis=1), validation[target_fields]\n",
        "test_features, test_targets = test.drop(target_fields, axis=1), test[target_fields]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XQWOF3WOf5Iq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the deep learning libraries\n",
        "\n",
        "Keras is a high level API built on TensorFlow, to build and test a neural network with minimal lines of code"
      ]
    },
    {
      "metadata": {
        "id": "r8AED3AYp8zA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zh97kDHWf5It",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import keras as kr\n",
        "import keras_metrics\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gVDmhFR0f5JC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Time to build the network\n",
        "\n",
        "The network has three layers, two hidden layer and one output layer. The hidden layers will use the sigmoid function for activations.the first hidden layers 19 neural and the second hidden layers has 16 neural.\n",
        "\n",
        "The output layer has only one node and is used for the classification **zero or one** so it also will use sigmoid function, the activation function is $f(x)=(1/(1 + \\exp(-x)))$. \n",
        "\n",
        "I Use **binary_crossentropy** because every output is independent, not mutually exclusive and can take values 0 or 1 \n",
        "Hy′(y):=−∑iy′ilog(yi) yi is the predicted probability value for class i and y′i is the true probability for that class.\n",
        "\n",
        "The **Adam optimization** algorithm is an extension to stochastic gradient descent to update network weights iterative based in training data.\n",
        "It has many benefits but what we need is that its  Appropriate for problems with very noisy/or sparse gradients, Well suited for problems that are large in terms of data and/or parameters and Little memory requirements.\n",
        "\n",
        "The method computes individual **adaptive learning rates** for different parameters from estimates of first and second moments of the gradients.\n"
      ]
    },
    {
      "metadata": {
        "id": "f9mAQb5yNzKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Precision and recall are then defined as\n",
        "\n",
        "$Precision=tp / (tp+fp)$. \n",
        "\n",
        "\n",
        "$Recall= tp/ (tp + fn)$. \n",
        "\n",
        "We included  using  to our test **keras_metrics**"
      ]
    },
    {
      "metadata": {
        "id": "uEmtn8Wdf5JK",
        "colab_type": "code",
        "outputId": "d552c995-a6cf-4749-9d75-9f5fae338d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(19, input_dim=train_features.shape[1]))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "learning_rate = 0.9\n",
        "momentum = 0.8\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=sgd, metrics=[keras_metrics.precision(), keras_metrics.recall(), 'accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 19)                418       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 19)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                320       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 755\n",
            "Trainable params: 755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gdaGz5urHBtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We included validation_data so **keras** make sure not to overfit"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "EfMHLwDbf5JT",
        "colab_type": "code",
        "outputId": "f55af240-b93c-4bf5-ad0d-37ff020fc9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17421
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features, train_targets, epochs=500,batch_size=1024,  validation_data=(validation_features, validation_targets))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 283028 samples, validate on 70757 samples\n",
            "Epoch 1/500\n",
            "283028/283028 [==============================] - 3s 11us/step - loss: 0.6473 - precision: 0.6091 - recall: 0.5324 - acc: 0.5979 - val_loss: 0.6350 - val_precision: 0.7702 - val_recall: 0.3268 - val_acc: 0.6176\n",
            "Epoch 2/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.6292 - precision: 0.6372 - recall: 0.5295 - acc: 0.6164 - val_loss: 0.6267 - val_precision: 0.5998 - val_recall: 0.6550 - val_acc: 0.6120\n",
            "Epoch 3/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.6261 - precision: 0.6397 - recall: 0.5413 - acc: 0.6206 - val_loss: 0.6255 - val_precision: 0.5930 - val_recall: 0.7620 - val_acc: 0.6224\n",
            "Epoch 4/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.6231 - precision: 0.6419 - recall: 0.5698 - acc: 0.6283 - val_loss: 0.6215 - val_precision: 0.6779 - val_recall: 0.4842 - val_acc: 0.6299\n",
            "Epoch 5/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.6206 - precision: 0.6429 - recall: 0.5855 - acc: 0.6324 - val_loss: 0.6193 - val_precision: 0.6183 - val_recall: 0.7123 - val_acc: 0.6390\n",
            "Epoch 6/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.6176 - precision: 0.6483 - recall: 0.6023 - acc: 0.6400 - val_loss: 0.6156 - val_precision: 0.6228 - val_recall: 0.7034 - val_acc: 0.6415\n",
            "Epoch 7/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.6149 - precision: 0.6500 - recall: 0.6130 - acc: 0.6437 - val_loss: 0.6143 - val_precision: 0.6948 - val_recall: 0.5069 - val_acc: 0.6448\n",
            "Epoch 8/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.6116 - precision: 0.6540 - recall: 0.6201 - acc: 0.6482 - val_loss: 0.6160 - val_precision: 0.6036 - val_recall: 0.7844 - val_acc: 0.6374\n",
            "Epoch 9/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.6102 - precision: 0.6556 - recall: 0.6222 - acc: 0.6498 - val_loss: 0.6078 - val_precision: 0.6804 - val_recall: 0.5644 - val_acc: 0.6523\n",
            "Epoch 10/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.6060 - precision: 0.6630 - recall: 0.6254 - acc: 0.6558 - val_loss: 0.6056 - val_precision: 0.6516 - val_recall: 0.6776 - val_acc: 0.6603\n",
            "Epoch 11/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.6036 - precision: 0.6634 - recall: 0.6326 - acc: 0.6579 - val_loss: 0.6078 - val_precision: 0.6244 - val_recall: 0.7688 - val_acc: 0.6558\n",
            "Epoch 12/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.6005 - precision: 0.6639 - recall: 0.6456 - acc: 0.6615 - val_loss: 0.5985 - val_precision: 0.6594 - val_recall: 0.6759 - val_acc: 0.6660\n",
            "Epoch 13/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5983 - precision: 0.6640 - recall: 0.6575 - acc: 0.6645 - val_loss: 0.5975 - val_precision: 0.6682 - val_recall: 0.6568 - val_acc: 0.6679\n",
            "Epoch 14/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5959 - precision: 0.6659 - recall: 0.6641 - acc: 0.6675 - val_loss: 0.5989 - val_precision: 0.6352 - val_recall: 0.7607 - val_acc: 0.6645\n",
            "Epoch 15/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5943 - precision: 0.6658 - recall: 0.6676 - acc: 0.6683 - val_loss: 0.6035 - val_precision: 0.6130 - val_recall: 0.8337 - val_acc: 0.6563\n",
            "Epoch 16/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5928 - precision: 0.6674 - recall: 0.6679 - acc: 0.6696 - val_loss: 0.5968 - val_precision: 0.6261 - val_recall: 0.7971 - val_acc: 0.6632\n",
            "Epoch 17/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5914 - precision: 0.6679 - recall: 0.6722 - acc: 0.6710 - val_loss: 0.5909 - val_precision: 0.6888 - val_recall: 0.6172 - val_acc: 0.6717\n",
            "Epoch 18/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5899 - precision: 0.6703 - recall: 0.6721 - acc: 0.6728 - val_loss: 0.5884 - val_precision: 0.6631 - val_recall: 0.6966 - val_acc: 0.6738\n",
            "Epoch 19/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5880 - precision: 0.6716 - recall: 0.6749 - acc: 0.6745 - val_loss: 0.5896 - val_precision: 0.6456 - val_recall: 0.7444 - val_acc: 0.6704\n",
            "Epoch 20/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.5866 - precision: 0.6731 - recall: 0.6730 - acc: 0.6751 - val_loss: 0.5863 - val_precision: 0.6581 - val_recall: 0.7202 - val_acc: 0.6755\n",
            "Epoch 21/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5848 - precision: 0.6761 - recall: 0.6740 - acc: 0.6776 - val_loss: 0.5845 - val_precision: 0.6536 - val_recall: 0.7308 - val_acc: 0.6743\n",
            "Epoch 22/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5826 - precision: 0.6792 - recall: 0.6730 - acc: 0.6795 - val_loss: 0.5824 - val_precision: 0.6982 - val_recall: 0.6417 - val_acc: 0.6846\n",
            "Epoch 23/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5814 - precision: 0.6798 - recall: 0.6724 - acc: 0.6798 - val_loss: 0.5825 - val_precision: 0.6647 - val_recall: 0.7031 - val_acc: 0.6767\n",
            "Epoch 24/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5801 - precision: 0.6823 - recall: 0.6707 - acc: 0.6812 - val_loss: 0.5816 - val_precision: 0.6630 - val_recall: 0.7233 - val_acc: 0.6803\n",
            "Epoch 25/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5782 - precision: 0.6841 - recall: 0.6730 - acc: 0.6831 - val_loss: 0.5784 - val_precision: 0.6986 - val_recall: 0.6363 - val_acc: 0.6833\n",
            "Epoch 26/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5774 - precision: 0.6863 - recall: 0.6708 - acc: 0.6841 - val_loss: 0.5782 - val_precision: 0.6670 - val_recall: 0.7245 - val_acc: 0.6838\n",
            "Epoch 27/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5756 - precision: 0.6876 - recall: 0.6709 - acc: 0.6850 - val_loss: 0.5755 - val_precision: 0.6769 - val_recall: 0.7064 - val_acc: 0.6870\n",
            "Epoch 28/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5745 - precision: 0.6878 - recall: 0.6746 - acc: 0.6861 - val_loss: 0.5741 - val_precision: 0.6879 - val_recall: 0.6823 - val_acc: 0.6887\n",
            "Epoch 29/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.5735 - precision: 0.6879 - recall: 0.6734 - acc: 0.6859 - val_loss: 0.5751 - val_precision: 0.7292 - val_recall: 0.5693 - val_acc: 0.6814\n",
            "Epoch 30/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5713 - precision: 0.6903 - recall: 0.6759 - acc: 0.6883 - val_loss: 0.5713 - val_precision: 0.6863 - val_recall: 0.6906 - val_acc: 0.6898\n",
            "Epoch 31/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5698 - precision: 0.6903 - recall: 0.6799 - acc: 0.6894 - val_loss: 0.5722 - val_precision: 0.6702 - val_recall: 0.7336 - val_acc: 0.6887\n",
            "Epoch 32/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5679 - precision: 0.6919 - recall: 0.6817 - acc: 0.6909 - val_loss: 0.5678 - val_precision: 0.7047 - val_recall: 0.6411 - val_acc: 0.6886\n",
            "Epoch 33/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5657 - precision: 0.6934 - recall: 0.6821 - acc: 0.6922 - val_loss: 0.5655 - val_precision: 0.7035 - val_recall: 0.6546 - val_acc: 0.6917\n",
            "Epoch 34/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5637 - precision: 0.6942 - recall: 0.6847 - acc: 0.6934 - val_loss: 0.5642 - val_precision: 0.7008 - val_recall: 0.6715 - val_acc: 0.6947\n",
            "Epoch 35/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5620 - precision: 0.6957 - recall: 0.6856 - acc: 0.6947 - val_loss: 0.5621 - val_precision: 0.6697 - val_recall: 0.7610 - val_acc: 0.6952\n",
            "Epoch 36/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5597 - precision: 0.6973 - recall: 0.6872 - acc: 0.6963 - val_loss: 0.5620 - val_precision: 0.7235 - val_recall: 0.6181 - val_acc: 0.6933\n",
            "Epoch 37/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5581 - precision: 0.6973 - recall: 0.6886 - acc: 0.6967 - val_loss: 0.5600 - val_precision: 0.6934 - val_recall: 0.6908 - val_acc: 0.6950\n",
            "Epoch 38/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5567 - precision: 0.6979 - recall: 0.6891 - acc: 0.6973 - val_loss: 0.5592 - val_precision: 0.6702 - val_recall: 0.7522 - val_acc: 0.6933\n",
            "Epoch 39/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5546 - precision: 0.6997 - recall: 0.6907 - acc: 0.6990 - val_loss: 0.5554 - val_precision: 0.7096 - val_recall: 0.6667 - val_acc: 0.6992\n",
            "Epoch 40/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5530 - precision: 0.7018 - recall: 0.6903 - acc: 0.7004 - val_loss: 0.5582 - val_precision: 0.7314 - val_recall: 0.6130 - val_acc: 0.6963\n",
            "Epoch 41/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5522 - precision: 0.7016 - recall: 0.6923 - acc: 0.7008 - val_loss: 0.5519 - val_precision: 0.7025 - val_recall: 0.6936 - val_acc: 0.7022\n",
            "Epoch 42/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5496 - precision: 0.7029 - recall: 0.6928 - acc: 0.7018 - val_loss: 0.5504 - val_precision: 0.6951 - val_recall: 0.7139 - val_acc: 0.7026\n",
            "Epoch 43/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5484 - precision: 0.7048 - recall: 0.6920 - acc: 0.7029 - val_loss: 0.5528 - val_precision: 0.6781 - val_recall: 0.7579 - val_acc: 0.7014\n",
            "Epoch 44/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5471 - precision: 0.7056 - recall: 0.6948 - acc: 0.7043 - val_loss: 0.5492 - val_precision: 0.6975 - val_recall: 0.7031 - val_acc: 0.7013\n",
            "Epoch 45/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5459 - precision: 0.7060 - recall: 0.6956 - acc: 0.7048 - val_loss: 0.5483 - val_precision: 0.7066 - val_recall: 0.6934 - val_acc: 0.7050\n",
            "Epoch 46/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5447 - precision: 0.7084 - recall: 0.6952 - acc: 0.7064 - val_loss: 0.5499 - val_precision: 0.7297 - val_recall: 0.6233 - val_acc: 0.6985\n",
            "Epoch 47/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5433 - precision: 0.7096 - recall: 0.6945 - acc: 0.7069 - val_loss: 0.5451 - val_precision: 0.7207 - val_recall: 0.6659 - val_acc: 0.7062\n",
            "Epoch 48/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.5422 - precision: 0.7115 - recall: 0.6943 - acc: 0.7082 - val_loss: 0.5471 - val_precision: 0.7361 - val_recall: 0.6225 - val_acc: 0.7020\n",
            "Epoch 49/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5421 - precision: 0.7113 - recall: 0.6947 - acc: 0.7081 - val_loss: 0.5424 - val_precision: 0.7123 - val_recall: 0.6866 - val_acc: 0.7069\n",
            "Epoch 50/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5401 - precision: 0.7129 - recall: 0.6955 - acc: 0.7095 - val_loss: 0.5424 - val_precision: 0.7339 - val_recall: 0.6381 - val_acc: 0.7056\n",
            "Epoch 51/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5390 - precision: 0.7138 - recall: 0.6949 - acc: 0.7099 - val_loss: 0.5397 - val_precision: 0.7220 - val_recall: 0.6807 - val_acc: 0.7115\n",
            "Epoch 52/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5381 - precision: 0.7156 - recall: 0.6954 - acc: 0.7113 - val_loss: 0.5395 - val_precision: 0.7255 - val_recall: 0.6668 - val_acc: 0.7095\n",
            "Epoch 53/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5373 - precision: 0.7159 - recall: 0.6947 - acc: 0.7113 - val_loss: 0.5422 - val_precision: 0.7323 - val_recall: 0.6479 - val_acc: 0.7078\n",
            "Epoch 54/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5358 - precision: 0.7170 - recall: 0.6977 - acc: 0.7130 - val_loss: 0.5380 - val_precision: 0.7330 - val_recall: 0.6540 - val_acc: 0.7101\n",
            "Epoch 55/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5345 - precision: 0.7175 - recall: 0.7006 - acc: 0.7142 - val_loss: 0.5356 - val_precision: 0.7270 - val_recall: 0.6766 - val_acc: 0.7135\n",
            "Epoch 56/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5339 - precision: 0.7187 - recall: 0.7006 - acc: 0.7150 - val_loss: 0.5379 - val_precision: 0.7375 - val_recall: 0.6452 - val_acc: 0.7100\n",
            "Epoch 57/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5327 - precision: 0.7178 - recall: 0.7044 - acc: 0.7155 - val_loss: 0.5356 - val_precision: 0.6983 - val_recall: 0.7480 - val_acc: 0.7146\n",
            "Epoch 58/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5319 - precision: 0.7183 - recall: 0.7064 - acc: 0.7164 - val_loss: 0.5367 - val_precision: 0.7273 - val_recall: 0.6619 - val_acc: 0.7091\n",
            "Epoch 59/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5311 - precision: 0.7182 - recall: 0.7079 - acc: 0.7168 - val_loss: 0.5346 - val_precision: 0.6981 - val_recall: 0.7332 - val_acc: 0.7103\n",
            "Epoch 60/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5298 - precision: 0.7185 - recall: 0.7117 - acc: 0.7182 - val_loss: 0.5358 - val_precision: 0.6847 - val_recall: 0.7873 - val_acc: 0.7145\n",
            "Epoch 61/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5292 - precision: 0.7195 - recall: 0.7112 - acc: 0.7187 - val_loss: 0.5306 - val_precision: 0.7150 - val_recall: 0.7112 - val_acc: 0.7160\n",
            "Epoch 62/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5282 - precision: 0.7196 - recall: 0.7134 - acc: 0.7195 - val_loss: 0.5311 - val_precision: 0.7434 - val_recall: 0.6502 - val_acc: 0.7152\n",
            "Epoch 63/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5279 - precision: 0.7195 - recall: 0.7129 - acc: 0.7193 - val_loss: 0.5328 - val_precision: 0.7499 - val_recall: 0.6294 - val_acc: 0.7119\n",
            "Epoch 64/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5270 - precision: 0.7192 - recall: 0.7145 - acc: 0.7195 - val_loss: 0.5289 - val_precision: 0.7222 - val_recall: 0.6968 - val_acc: 0.7166\n",
            "Epoch 65/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5264 - precision: 0.7197 - recall: 0.7142 - acc: 0.7197 - val_loss: 0.5270 - val_precision: 0.7084 - val_recall: 0.7419 - val_acc: 0.7204\n",
            "Epoch 66/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5259 - precision: 0.7188 - recall: 0.7165 - acc: 0.7198 - val_loss: 0.5286 - val_precision: 0.7116 - val_recall: 0.7221 - val_acc: 0.7169\n",
            "Epoch 67/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5250 - precision: 0.7193 - recall: 0.7157 - acc: 0.7200 - val_loss: 0.5270 - val_precision: 0.7028 - val_recall: 0.7590 - val_acc: 0.7212\n",
            "Epoch 68/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5249 - precision: 0.7194 - recall: 0.7170 - acc: 0.7204 - val_loss: 0.5275 - val_precision: 0.7045 - val_recall: 0.7558 - val_acc: 0.7215\n",
            "Epoch 69/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5242 - precision: 0.7186 - recall: 0.7206 - acc: 0.7209 - val_loss: 0.5298 - val_precision: 0.7291 - val_recall: 0.6814 - val_acc: 0.7163\n",
            "Epoch 70/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5233 - precision: 0.7196 - recall: 0.7197 - acc: 0.7213 - val_loss: 0.5318 - val_precision: 0.7444 - val_recall: 0.6339 - val_acc: 0.7104\n",
            "Epoch 71/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5234 - precision: 0.7186 - recall: 0.7198 - acc: 0.7207 - val_loss: 0.5291 - val_precision: 0.7539 - val_recall: 0.6300 - val_acc: 0.7144\n",
            "Epoch 72/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5222 - precision: 0.7198 - recall: 0.7198 - acc: 0.7215 - val_loss: 0.5247 - val_precision: 0.7007 - val_recall: 0.7612 - val_acc: 0.7202\n",
            "Epoch 73/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5218 - precision: 0.7198 - recall: 0.7207 - acc: 0.7218 - val_loss: 0.5242 - val_precision: 0.7183 - val_recall: 0.7184 - val_acc: 0.7205\n",
            "Epoch 74/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5215 - precision: 0.7194 - recall: 0.7222 - acc: 0.7219 - val_loss: 0.5225 - val_precision: 0.7186 - val_recall: 0.7270 - val_acc: 0.7233\n",
            "Epoch 75/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5205 - precision: 0.7209 - recall: 0.7221 - acc: 0.7230 - val_loss: 0.5229 - val_precision: 0.7030 - val_recall: 0.7625 - val_acc: 0.7223\n",
            "Epoch 76/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5204 - precision: 0.7199 - recall: 0.7228 - acc: 0.7225 - val_loss: 0.5217 - val_precision: 0.7218 - val_recall: 0.7095 - val_acc: 0.7201\n",
            "Epoch 77/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5205 - precision: 0.7208 - recall: 0.7226 - acc: 0.7231 - val_loss: 0.5229 - val_precision: 0.7524 - val_recall: 0.6455 - val_acc: 0.7187\n",
            "Epoch 78/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5196 - precision: 0.7217 - recall: 0.7218 - acc: 0.7235 - val_loss: 0.5223 - val_precision: 0.6956 - val_recall: 0.7746 - val_acc: 0.7200\n",
            "Epoch 79/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5191 - precision: 0.7219 - recall: 0.7222 - acc: 0.7237 - val_loss: 0.5202 - val_precision: 0.7099 - val_recall: 0.7514 - val_acc: 0.7243\n",
            "Epoch 80/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5184 - precision: 0.7216 - recall: 0.7236 - acc: 0.7239 - val_loss: 0.5206 - val_precision: 0.7115 - val_recall: 0.7464 - val_acc: 0.7240\n",
            "Epoch 81/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5183 - precision: 0.7224 - recall: 0.7245 - acc: 0.7247 - val_loss: 0.5207 - val_precision: 0.7387 - val_recall: 0.6903 - val_acc: 0.7251\n",
            "Epoch 82/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5184 - precision: 0.7220 - recall: 0.7242 - acc: 0.7244 - val_loss: 0.5178 - val_precision: 0.7240 - val_recall: 0.7227 - val_acc: 0.7257\n",
            "Epoch 83/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5174 - precision: 0.7235 - recall: 0.7233 - acc: 0.7251 - val_loss: 0.5187 - val_precision: 0.7335 - val_recall: 0.6974 - val_acc: 0.7241\n",
            "Epoch 84/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5173 - precision: 0.7231 - recall: 0.7240 - acc: 0.7251 - val_loss: 0.5175 - val_precision: 0.7132 - val_recall: 0.7542 - val_acc: 0.7275\n",
            "Epoch 85/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5164 - precision: 0.7233 - recall: 0.7244 - acc: 0.7254 - val_loss: 0.5176 - val_precision: 0.7125 - val_recall: 0.7484 - val_acc: 0.7253\n",
            "Epoch 86/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5158 - precision: 0.7235 - recall: 0.7243 - acc: 0.7255 - val_loss: 0.5166 - val_precision: 0.7230 - val_recall: 0.7201 - val_acc: 0.7242\n",
            "Epoch 87/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5155 - precision: 0.7240 - recall: 0.7245 - acc: 0.7259 - val_loss: 0.5188 - val_precision: 0.7174 - val_recall: 0.7259 - val_acc: 0.7221\n",
            "Epoch 88/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5157 - precision: 0.7241 - recall: 0.7244 - acc: 0.7259 - val_loss: 0.5165 - val_precision: 0.7219 - val_recall: 0.7225 - val_acc: 0.7242\n",
            "Epoch 89/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5150 - precision: 0.7246 - recall: 0.7253 - acc: 0.7265 - val_loss: 0.5175 - val_precision: 0.6994 - val_recall: 0.7745 - val_acc: 0.7229\n",
            "Epoch 90/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5146 - precision: 0.7251 - recall: 0.7246 - acc: 0.7267 - val_loss: 0.5153 - val_precision: 0.7290 - val_recall: 0.7171 - val_acc: 0.7273\n",
            "Epoch 91/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5137 - precision: 0.7262 - recall: 0.7270 - acc: 0.7281 - val_loss: 0.5171 - val_precision: 0.7354 - val_recall: 0.6935 - val_acc: 0.7241\n",
            "Epoch 92/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5135 - precision: 0.7263 - recall: 0.7247 - acc: 0.7275 - val_loss: 0.5157 - val_precision: 0.7157 - val_recall: 0.7396 - val_acc: 0.7250\n",
            "Epoch 93/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5132 - precision: 0.7255 - recall: 0.7264 - acc: 0.7275 - val_loss: 0.5183 - val_precision: 0.7456 - val_recall: 0.6607 - val_acc: 0.7197\n",
            "Epoch 94/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5130 - precision: 0.7265 - recall: 0.7242 - acc: 0.7274 - val_loss: 0.5169 - val_precision: 0.7174 - val_recall: 0.7424 - val_acc: 0.7271\n",
            "Epoch 95/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5124 - precision: 0.7270 - recall: 0.7259 - acc: 0.7283 - val_loss: 0.5184 - val_precision: 0.7639 - val_recall: 0.6289 - val_acc: 0.7194\n",
            "Epoch 96/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5118 - precision: 0.7283 - recall: 0.7238 - acc: 0.7286 - val_loss: 0.5129 - val_precision: 0.7321 - val_recall: 0.7106 - val_acc: 0.7273\n",
            "Epoch 97/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5113 - precision: 0.7291 - recall: 0.7234 - acc: 0.7290 - val_loss: 0.5122 - val_precision: 0.7308 - val_recall: 0.7130 - val_acc: 0.7273\n",
            "Epoch 98/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5111 - precision: 0.7282 - recall: 0.7220 - acc: 0.7280 - val_loss: 0.5128 - val_precision: 0.7211 - val_recall: 0.7231 - val_acc: 0.7238\n",
            "Epoch 99/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5106 - precision: 0.7289 - recall: 0.7237 - acc: 0.7290 - val_loss: 0.5151 - val_precision: 0.7259 - val_recall: 0.7234 - val_acc: 0.7272\n",
            "Epoch 100/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5109 - precision: 0.7291 - recall: 0.7231 - acc: 0.7289 - val_loss: 0.5134 - val_precision: 0.7428 - val_recall: 0.6819 - val_acc: 0.7250\n",
            "Epoch 101/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5097 - precision: 0.7300 - recall: 0.7227 - acc: 0.7293 - val_loss: 0.5154 - val_precision: 0.7282 - val_recall: 0.7155 - val_acc: 0.7263\n",
            "Epoch 102/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5095 - precision: 0.7304 - recall: 0.7235 - acc: 0.7299 - val_loss: 0.5114 - val_precision: 0.7388 - val_recall: 0.6991 - val_acc: 0.7280\n",
            "Epoch 103/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5089 - precision: 0.7303 - recall: 0.7206 - acc: 0.7289 - val_loss: 0.5100 - val_precision: 0.7189 - val_recall: 0.7516 - val_acc: 0.7309\n",
            "Epoch 104/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5082 - precision: 0.7304 - recall: 0.7222 - acc: 0.7295 - val_loss: 0.5084 - val_precision: 0.7335 - val_recall: 0.7117 - val_acc: 0.7286\n",
            "Epoch 105/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5078 - precision: 0.7319 - recall: 0.7234 - acc: 0.7309 - val_loss: 0.5120 - val_precision: 0.7059 - val_recall: 0.7753 - val_acc: 0.7283\n",
            "Epoch 106/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5075 - precision: 0.7311 - recall: 0.7223 - acc: 0.7300 - val_loss: 0.5103 - val_precision: 0.7418 - val_recall: 0.6986 - val_acc: 0.7298\n",
            "Epoch 107/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5068 - precision: 0.7315 - recall: 0.7227 - acc: 0.7304 - val_loss: 0.5105 - val_precision: 0.7332 - val_recall: 0.7187 - val_acc: 0.7306\n",
            "Epoch 108/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5066 - precision: 0.7317 - recall: 0.7228 - acc: 0.7305 - val_loss: 0.5109 - val_precision: 0.7103 - val_recall: 0.7677 - val_acc: 0.7294\n",
            "Epoch 109/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5059 - precision: 0.7315 - recall: 0.7239 - acc: 0.7308 - val_loss: 0.5074 - val_precision: 0.7435 - val_recall: 0.6958 - val_acc: 0.7299\n",
            "Epoch 110/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5056 - precision: 0.7324 - recall: 0.7234 - acc: 0.7312 - val_loss: 0.5131 - val_precision: 0.6986 - val_recall: 0.7938 - val_acc: 0.7278\n",
            "Epoch 111/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5054 - precision: 0.7316 - recall: 0.7241 - acc: 0.7309 - val_loss: 0.5136 - val_precision: 0.7628 - val_recall: 0.6437 - val_acc: 0.7239\n",
            "Epoch 112/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5051 - precision: 0.7318 - recall: 0.7253 - acc: 0.7314 - val_loss: 0.5061 - val_precision: 0.7417 - val_recall: 0.6983 - val_acc: 0.7296\n",
            "Epoch 113/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5050 - precision: 0.7322 - recall: 0.7248 - acc: 0.7315 - val_loss: 0.5090 - val_precision: 0.7324 - val_recall: 0.7131 - val_acc: 0.7283\n",
            "Epoch 114/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5042 - precision: 0.7326 - recall: 0.7243 - acc: 0.7316 - val_loss: 0.5052 - val_precision: 0.7412 - val_recall: 0.6889 - val_acc: 0.7263\n",
            "Epoch 115/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5038 - precision: 0.7329 - recall: 0.7237 - acc: 0.7316 - val_loss: 0.5065 - val_precision: 0.7267 - val_recall: 0.7371 - val_acc: 0.7320\n",
            "Epoch 116/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.5030 - precision: 0.7333 - recall: 0.7256 - acc: 0.7325 - val_loss: 0.5034 - val_precision: 0.7345 - val_recall: 0.7118 - val_acc: 0.7293\n",
            "Epoch 117/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5027 - precision: 0.7341 - recall: 0.7244 - acc: 0.7327 - val_loss: 0.5167 - val_precision: 0.6798 - val_recall: 0.8347 - val_acc: 0.7229\n",
            "Epoch 118/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5029 - precision: 0.7326 - recall: 0.7267 - acc: 0.7324 - val_loss: 0.5092 - val_precision: 0.7509 - val_recall: 0.6646 - val_acc: 0.7242\n",
            "Epoch 119/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5021 - precision: 0.7332 - recall: 0.7260 - acc: 0.7326 - val_loss: 0.5036 - val_precision: 0.7274 - val_recall: 0.7345 - val_acc: 0.7317\n",
            "Epoch 120/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5014 - precision: 0.7347 - recall: 0.7258 - acc: 0.7335 - val_loss: 0.5062 - val_precision: 0.7556 - val_recall: 0.6569 - val_acc: 0.7243\n",
            "Epoch 121/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5019 - precision: 0.7334 - recall: 0.7263 - acc: 0.7328 - val_loss: 0.5053 - val_precision: 0.7071 - val_recall: 0.7812 - val_acc: 0.7308\n",
            "Epoch 122/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5015 - precision: 0.7335 - recall: 0.7267 - acc: 0.7330 - val_loss: 0.5036 - val_precision: 0.7480 - val_recall: 0.6819 - val_acc: 0.7282\n",
            "Epoch 123/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5010 - precision: 0.7339 - recall: 0.7274 - acc: 0.7335 - val_loss: 0.5032 - val_precision: 0.7435 - val_recall: 0.7043 - val_acc: 0.7327\n",
            "Epoch 124/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5013 - precision: 0.7341 - recall: 0.7266 - acc: 0.7333 - val_loss: 0.5080 - val_precision: 0.6991 - val_recall: 0.7970 - val_acc: 0.7291\n",
            "Epoch 125/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5007 - precision: 0.7338 - recall: 0.7289 - acc: 0.7339 - val_loss: 0.5002 - val_precision: 0.7299 - val_recall: 0.7322 - val_acc: 0.7327\n",
            "Epoch 126/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5003 - precision: 0.7341 - recall: 0.7280 - acc: 0.7338 - val_loss: 0.5061 - val_precision: 0.7516 - val_recall: 0.6722 - val_acc: 0.7271\n",
            "Epoch 127/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.5006 - precision: 0.7345 - recall: 0.7294 - acc: 0.7345 - val_loss: 0.5012 - val_precision: 0.7408 - val_recall: 0.7077 - val_acc: 0.7321\n",
            "Epoch 128/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.5002 - precision: 0.7339 - recall: 0.7301 - acc: 0.7343 - val_loss: 0.5052 - val_precision: 0.7524 - val_recall: 0.6702 - val_acc: 0.7269\n",
            "Epoch 129/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4997 - precision: 0.7342 - recall: 0.7307 - acc: 0.7347 - val_loss: 0.5011 - val_precision: 0.7286 - val_recall: 0.7439 - val_acc: 0.7354\n",
            "Epoch 130/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4991 - precision: 0.7355 - recall: 0.7307 - acc: 0.7356 - val_loss: 0.5002 - val_precision: 0.7267 - val_recall: 0.7506 - val_acc: 0.7362\n",
            "Epoch 131/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4991 - precision: 0.7345 - recall: 0.7312 - acc: 0.7351 - val_loss: 0.5040 - val_precision: 0.7249 - val_recall: 0.7342 - val_acc: 0.7299\n",
            "Epoch 132/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4987 - precision: 0.7342 - recall: 0.7311 - acc: 0.7348 - val_loss: 0.5046 - val_precision: 0.7080 - val_recall: 0.7778 - val_acc: 0.7306\n",
            "Epoch 133/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4988 - precision: 0.7347 - recall: 0.7324 - acc: 0.7356 - val_loss: 0.5003 - val_precision: 0.7455 - val_recall: 0.6921 - val_acc: 0.7300\n",
            "Epoch 134/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4981 - precision: 0.7358 - recall: 0.7330 - acc: 0.7366 - val_loss: 0.5036 - val_precision: 0.7400 - val_recall: 0.7167 - val_acc: 0.7345\n",
            "Epoch 135/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4979 - precision: 0.7345 - recall: 0.7354 - acc: 0.7364 - val_loss: 0.4986 - val_precision: 0.7425 - val_recall: 0.7042 - val_acc: 0.7321\n",
            "Epoch 136/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4976 - precision: 0.7358 - recall: 0.7319 - acc: 0.7362 - val_loss: 0.5010 - val_precision: 0.7137 - val_recall: 0.7692 - val_acc: 0.7324\n",
            "Epoch 137/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4978 - precision: 0.7345 - recall: 0.7353 - acc: 0.7364 - val_loss: 0.4985 - val_precision: 0.7488 - val_recall: 0.6915 - val_acc: 0.7318\n",
            "Epoch 138/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4970 - precision: 0.7367 - recall: 0.7359 - acc: 0.7381 - val_loss: 0.5001 - val_precision: 0.7266 - val_recall: 0.7500 - val_acc: 0.7359\n",
            "Epoch 139/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4973 - precision: 0.7345 - recall: 0.7351 - acc: 0.7363 - val_loss: 0.4989 - val_precision: 0.7164 - val_recall: 0.7644 - val_acc: 0.7330\n",
            "Epoch 140/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4967 - precision: 0.7357 - recall: 0.7346 - acc: 0.7369 - val_loss: 0.4979 - val_precision: 0.7394 - val_recall: 0.7286 - val_acc: 0.7379\n",
            "Epoch 141/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4962 - precision: 0.7353 - recall: 0.7378 - acc: 0.7378 - val_loss: 0.4965 - val_precision: 0.7390 - val_recall: 0.7356 - val_acc: 0.7399\n",
            "Epoch 142/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4963 - precision: 0.7357 - recall: 0.7363 - acc: 0.7375 - val_loss: 0.4959 - val_precision: 0.7456 - val_recall: 0.7067 - val_acc: 0.7348\n",
            "Epoch 143/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4957 - precision: 0.7362 - recall: 0.7365 - acc: 0.7379 - val_loss: 0.4974 - val_precision: 0.7316 - val_recall: 0.7408 - val_acc: 0.7365\n",
            "Epoch 144/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4953 - precision: 0.7362 - recall: 0.7390 - acc: 0.7387 - val_loss: 0.4980 - val_precision: 0.7462 - val_recall: 0.7083 - val_acc: 0.7357\n",
            "Epoch 145/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4949 - precision: 0.7365 - recall: 0.7384 - acc: 0.7388 - val_loss: 0.4961 - val_precision: 0.7235 - val_recall: 0.7650 - val_acc: 0.7384\n",
            "Epoch 146/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4948 - precision: 0.7354 - recall: 0.7393 - acc: 0.7383 - val_loss: 0.4975 - val_precision: 0.7444 - val_recall: 0.7076 - val_acc: 0.7344\n",
            "Epoch 147/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4944 - precision: 0.7373 - recall: 0.7370 - acc: 0.7389 - val_loss: 0.4995 - val_precision: 0.7142 - val_recall: 0.7824 - val_acc: 0.7367\n",
            "Epoch 148/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4948 - precision: 0.7357 - recall: 0.7388 - acc: 0.7383 - val_loss: 0.4975 - val_precision: 0.7320 - val_recall: 0.7456 - val_acc: 0.7383\n",
            "Epoch 149/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4942 - precision: 0.7369 - recall: 0.7405 - acc: 0.7397 - val_loss: 0.4975 - val_precision: 0.7449 - val_recall: 0.6988 - val_acc: 0.7318\n",
            "Epoch 150/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4938 - precision: 0.7369 - recall: 0.7405 - acc: 0.7397 - val_loss: 0.4961 - val_precision: 0.7491 - val_recall: 0.6977 - val_acc: 0.7340\n",
            "Epoch 151/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4936 - precision: 0.7370 - recall: 0.7403 - acc: 0.7396 - val_loss: 0.4951 - val_precision: 0.7288 - val_recall: 0.7518 - val_acc: 0.7381\n",
            "Epoch 152/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4933 - precision: 0.7372 - recall: 0.7410 - acc: 0.7400 - val_loss: 0.4939 - val_precision: 0.7477 - val_recall: 0.7163 - val_acc: 0.7393\n",
            "Epoch 153/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4930 - precision: 0.7371 - recall: 0.7404 - acc: 0.7398 - val_loss: 0.4960 - val_precision: 0.7171 - val_recall: 0.7886 - val_acc: 0.7407\n",
            "Epoch 154/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4930 - precision: 0.7364 - recall: 0.7417 - acc: 0.7397 - val_loss: 0.4944 - val_precision: 0.7438 - val_recall: 0.7170 - val_acc: 0.7370\n",
            "Epoch 155/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4928 - precision: 0.7370 - recall: 0.7421 - acc: 0.7402 - val_loss: 0.4955 - val_precision: 0.7318 - val_recall: 0.7488 - val_acc: 0.7392\n",
            "Epoch 156/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4922 - precision: 0.7369 - recall: 0.7427 - acc: 0.7404 - val_loss: 0.5016 - val_precision: 0.7225 - val_recall: 0.7725 - val_acc: 0.7399\n",
            "Epoch 157/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4923 - precision: 0.7369 - recall: 0.7422 - acc: 0.7402 - val_loss: 0.4941 - val_precision: 0.7368 - val_recall: 0.7401 - val_acc: 0.7399\n",
            "Epoch 158/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4920 - precision: 0.7367 - recall: 0.7427 - acc: 0.7403 - val_loss: 0.4925 - val_precision: 0.7354 - val_recall: 0.7364 - val_acc: 0.7377\n",
            "Epoch 159/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4917 - precision: 0.7369 - recall: 0.7446 - acc: 0.7410 - val_loss: 0.4944 - val_precision: 0.7491 - val_recall: 0.7048 - val_acc: 0.7364\n",
            "Epoch 160/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4915 - precision: 0.7368 - recall: 0.7439 - acc: 0.7407 - val_loss: 0.4935 - val_precision: 0.7374 - val_recall: 0.7322 - val_acc: 0.7377\n",
            "Epoch 161/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4911 - precision: 0.7373 - recall: 0.7427 - acc: 0.7406 - val_loss: 0.4907 - val_precision: 0.7395 - val_recall: 0.7453 - val_acc: 0.7433\n",
            "Epoch 162/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4912 - precision: 0.7369 - recall: 0.7447 - acc: 0.7410 - val_loss: 0.4916 - val_precision: 0.7446 - val_recall: 0.7317 - val_acc: 0.7423\n",
            "Epoch 163/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4908 - precision: 0.7376 - recall: 0.7451 - acc: 0.7416 - val_loss: 0.4925 - val_precision: 0.7307 - val_recall: 0.7579 - val_acc: 0.7413\n",
            "Epoch 164/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4906 - precision: 0.7385 - recall: 0.7438 - acc: 0.7418 - val_loss: 0.4906 - val_precision: 0.7398 - val_recall: 0.7443 - val_acc: 0.7432\n",
            "Epoch 165/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4900 - precision: 0.7376 - recall: 0.7467 - acc: 0.7421 - val_loss: 0.4931 - val_precision: 0.7330 - val_recall: 0.7430 - val_acc: 0.7382\n",
            "Epoch 166/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4900 - precision: 0.7375 - recall: 0.7463 - acc: 0.7419 - val_loss: 0.4919 - val_precision: 0.7441 - val_recall: 0.7380 - val_acc: 0.7441\n",
            "Epoch 167/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4899 - precision: 0.7377 - recall: 0.7471 - acc: 0.7423 - val_loss: 0.4921 - val_precision: 0.7491 - val_recall: 0.7195 - val_acc: 0.7412\n",
            "Epoch 168/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4899 - precision: 0.7373 - recall: 0.7466 - acc: 0.7419 - val_loss: 0.4919 - val_precision: 0.7300 - val_recall: 0.7572 - val_acc: 0.7406\n",
            "Epoch 169/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4887 - precision: 0.7381 - recall: 0.7485 - acc: 0.7431 - val_loss: 0.4955 - val_precision: 0.7592 - val_recall: 0.6767 - val_acc: 0.7331\n",
            "Epoch 170/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4895 - precision: 0.7374 - recall: 0.7460 - acc: 0.7418 - val_loss: 0.4927 - val_precision: 0.7267 - val_recall: 0.7614 - val_acc: 0.7395\n",
            "Epoch 171/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4889 - precision: 0.7377 - recall: 0.7477 - acc: 0.7425 - val_loss: 0.4909 - val_precision: 0.7255 - val_recall: 0.7667 - val_acc: 0.7403\n",
            "Epoch 172/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4889 - precision: 0.7365 - recall: 0.7485 - acc: 0.7420 - val_loss: 0.4904 - val_precision: 0.7369 - val_recall: 0.7485 - val_acc: 0.7426\n",
            "Epoch 173/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4881 - precision: 0.7378 - recall: 0.7487 - acc: 0.7429 - val_loss: 0.4896 - val_precision: 0.7498 - val_recall: 0.7154 - val_acc: 0.7403\n",
            "Epoch 174/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4878 - precision: 0.7373 - recall: 0.7473 - acc: 0.7421 - val_loss: 0.4932 - val_precision: 0.7392 - val_recall: 0.7302 - val_acc: 0.7383\n",
            "Epoch 175/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4877 - precision: 0.7377 - recall: 0.7497 - acc: 0.7431 - val_loss: 0.4913 - val_precision: 0.7471 - val_recall: 0.7285 - val_acc: 0.7430\n",
            "Epoch 176/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4875 - precision: 0.7377 - recall: 0.7496 - acc: 0.7432 - val_loss: 0.4885 - val_precision: 0.7288 - val_recall: 0.7696 - val_acc: 0.7436\n",
            "Epoch 177/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4871 - precision: 0.7379 - recall: 0.7492 - acc: 0.7432 - val_loss: 0.4916 - val_precision: 0.7558 - val_recall: 0.7024 - val_acc: 0.7397\n",
            "Epoch 178/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4870 - precision: 0.7384 - recall: 0.7488 - acc: 0.7433 - val_loss: 0.4891 - val_precision: 0.7424 - val_recall: 0.7332 - val_acc: 0.7414\n",
            "Epoch 179/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4867 - precision: 0.7380 - recall: 0.7495 - acc: 0.7433 - val_loss: 0.4883 - val_precision: 0.7520 - val_recall: 0.7215 - val_acc: 0.7437\n",
            "Epoch 180/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4866 - precision: 0.7388 - recall: 0.7496 - acc: 0.7439 - val_loss: 0.4901 - val_precision: 0.7265 - val_recall: 0.7658 - val_acc: 0.7407\n",
            "Epoch 181/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4868 - precision: 0.7376 - recall: 0.7492 - acc: 0.7429 - val_loss: 0.4915 - val_precision: 0.7208 - val_recall: 0.7672 - val_acc: 0.7370\n",
            "Epoch 182/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4865 - precision: 0.7381 - recall: 0.7492 - acc: 0.7433 - val_loss: 0.4890 - val_precision: 0.7289 - val_recall: 0.7639 - val_acc: 0.7419\n",
            "Epoch 183/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4860 - precision: 0.7372 - recall: 0.7511 - acc: 0.7433 - val_loss: 0.4901 - val_precision: 0.7443 - val_recall: 0.7212 - val_acc: 0.7388\n",
            "Epoch 184/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4859 - precision: 0.7383 - recall: 0.7486 - acc: 0.7432 - val_loss: 0.4921 - val_precision: 0.7132 - val_recall: 0.7997 - val_acc: 0.7410\n",
            "Epoch 185/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4857 - precision: 0.7380 - recall: 0.7508 - acc: 0.7437 - val_loss: 0.4901 - val_precision: 0.7119 - val_recall: 0.8006 - val_acc: 0.7403\n",
            "Epoch 186/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4853 - precision: 0.7382 - recall: 0.7508 - acc: 0.7438 - val_loss: 0.4884 - val_precision: 0.7240 - val_recall: 0.7725 - val_acc: 0.7410\n",
            "Epoch 187/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4850 - precision: 0.7383 - recall: 0.7526 - acc: 0.7445 - val_loss: 0.4861 - val_precision: 0.7337 - val_recall: 0.7630 - val_acc: 0.7450\n",
            "Epoch 188/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4853 - precision: 0.7381 - recall: 0.7514 - acc: 0.7440 - val_loss: 0.4876 - val_precision: 0.7227 - val_recall: 0.7855 - val_acc: 0.7440\n",
            "Epoch 189/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4846 - precision: 0.7388 - recall: 0.7519 - acc: 0.7446 - val_loss: 0.4854 - val_precision: 0.7416 - val_recall: 0.7401 - val_acc: 0.7431\n",
            "Epoch 190/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4845 - precision: 0.7384 - recall: 0.7512 - acc: 0.7441 - val_loss: 0.4881 - val_precision: 0.7285 - val_recall: 0.7731 - val_acc: 0.7444\n",
            "Epoch 191/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4839 - precision: 0.7391 - recall: 0.7536 - acc: 0.7454 - val_loss: 0.4843 - val_precision: 0.7360 - val_recall: 0.7613 - val_acc: 0.7461\n",
            "Epoch 192/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4840 - precision: 0.7384 - recall: 0.7526 - acc: 0.7446 - val_loss: 0.4890 - val_precision: 0.7527 - val_recall: 0.7126 - val_acc: 0.7412\n",
            "Epoch 193/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4842 - precision: 0.7380 - recall: 0.7513 - acc: 0.7439 - val_loss: 0.4862 - val_precision: 0.7491 - val_recall: 0.7246 - val_acc: 0.7430\n",
            "Epoch 194/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4834 - precision: 0.7384 - recall: 0.7534 - acc: 0.7448 - val_loss: 0.4881 - val_precision: 0.7592 - val_recall: 0.6991 - val_acc: 0.7407\n",
            "Epoch 195/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4835 - precision: 0.7389 - recall: 0.7518 - acc: 0.7446 - val_loss: 0.4869 - val_precision: 0.7315 - val_recall: 0.7607 - val_acc: 0.7427\n",
            "Epoch 196/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4834 - precision: 0.7387 - recall: 0.7544 - acc: 0.7453 - val_loss: 0.4840 - val_precision: 0.7275 - val_recall: 0.7814 - val_acc: 0.7463\n",
            "Epoch 197/500\n",
            "283028/283028 [==============================] - 3s 11us/step - loss: 0.4828 - precision: 0.7384 - recall: 0.7537 - acc: 0.7449 - val_loss: 0.4860 - val_precision: 0.7404 - val_recall: 0.7473 - val_acc: 0.7446\n",
            "Epoch 198/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4828 - precision: 0.7387 - recall: 0.7540 - acc: 0.7452 - val_loss: 0.4881 - val_precision: 0.7599 - val_recall: 0.6950 - val_acc: 0.7397\n",
            "Epoch 199/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4828 - precision: 0.7390 - recall: 0.7537 - acc: 0.7453 - val_loss: 0.4859 - val_precision: 0.7321 - val_recall: 0.7598 - val_acc: 0.7429\n",
            "Epoch 200/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4826 - precision: 0.7384 - recall: 0.7552 - acc: 0.7454 - val_loss: 0.4872 - val_precision: 0.7164 - val_recall: 0.7954 - val_acc: 0.7423\n",
            "Epoch 201/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4829 - precision: 0.7383 - recall: 0.7537 - acc: 0.7449 - val_loss: 0.4827 - val_precision: 0.7303 - val_recall: 0.7705 - val_acc: 0.7450\n",
            "Epoch 202/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4823 - precision: 0.7379 - recall: 0.7546 - acc: 0.7448 - val_loss: 0.4950 - val_precision: 0.6966 - val_recall: 0.8442 - val_acc: 0.7403\n",
            "Epoch 203/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4821 - precision: 0.7384 - recall: 0.7552 - acc: 0.7454 - val_loss: 0.4837 - val_precision: 0.7435 - val_recall: 0.7378 - val_acc: 0.7436\n",
            "Epoch 204/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4818 - precision: 0.7391 - recall: 0.7573 - acc: 0.7466 - val_loss: 0.4870 - val_precision: 0.7175 - val_recall: 0.8000 - val_acc: 0.7445\n",
            "Epoch 205/500\n",
            "283028/283028 [==============================] - 3s 11us/step - loss: 0.4815 - precision: 0.7400 - recall: 0.7568 - acc: 0.7470 - val_loss: 0.4835 - val_precision: 0.7291 - val_recall: 0.7785 - val_acc: 0.7466\n",
            "Epoch 206/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4817 - precision: 0.7393 - recall: 0.7551 - acc: 0.7460 - val_loss: 0.4889 - val_precision: 0.7164 - val_recall: 0.7975 - val_acc: 0.7429\n",
            "Epoch 207/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4814 - precision: 0.7395 - recall: 0.7552 - acc: 0.7461 - val_loss: 0.4821 - val_precision: 0.7355 - val_recall: 0.7698 - val_acc: 0.7484\n",
            "Epoch 208/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4812 - precision: 0.7390 - recall: 0.7573 - acc: 0.7465 - val_loss: 0.4829 - val_precision: 0.7217 - val_recall: 0.7809 - val_acc: 0.7419\n",
            "Epoch 209/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4810 - precision: 0.7380 - recall: 0.7580 - acc: 0.7460 - val_loss: 0.4823 - val_precision: 0.7432 - val_recall: 0.7439 - val_acc: 0.7454\n",
            "Epoch 210/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4809 - precision: 0.7392 - recall: 0.7569 - acc: 0.7465 - val_loss: 0.4833 - val_precision: 0.7380 - val_recall: 0.7541 - val_acc: 0.7451\n",
            "Epoch 211/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4811 - precision: 0.7391 - recall: 0.7563 - acc: 0.7462 - val_loss: 0.4818 - val_precision: 0.7425 - val_recall: 0.7415 - val_acc: 0.7443\n",
            "Epoch 212/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4804 - precision: 0.7390 - recall: 0.7570 - acc: 0.7464 - val_loss: 0.4822 - val_precision: 0.7415 - val_recall: 0.7477 - val_acc: 0.7455\n",
            "Epoch 213/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4806 - precision: 0.7392 - recall: 0.7565 - acc: 0.7464 - val_loss: 0.4807 - val_precision: 0.7356 - val_recall: 0.7603 - val_acc: 0.7455\n",
            "Epoch 214/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4800 - precision: 0.7401 - recall: 0.7575 - acc: 0.7473 - val_loss: 0.4835 - val_precision: 0.7267 - val_recall: 0.7773 - val_acc: 0.7444\n",
            "Epoch 215/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4802 - precision: 0.7394 - recall: 0.7568 - acc: 0.7466 - val_loss: 0.4820 - val_precision: 0.7237 - val_recall: 0.7888 - val_acc: 0.7458\n",
            "Epoch 216/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4802 - precision: 0.7396 - recall: 0.7575 - acc: 0.7470 - val_loss: 0.4817 - val_precision: 0.7187 - val_recall: 0.7968 - val_acc: 0.7445\n",
            "Epoch 217/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4797 - precision: 0.7395 - recall: 0.7576 - acc: 0.7469 - val_loss: 0.4812 - val_precision: 0.7470 - val_recall: 0.7415 - val_acc: 0.7471\n",
            "Epoch 218/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4799 - precision: 0.7389 - recall: 0.7582 - acc: 0.7467 - val_loss: 0.4808 - val_precision: 0.7506 - val_recall: 0.7298 - val_acc: 0.7456\n",
            "Epoch 219/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4797 - precision: 0.7400 - recall: 0.7582 - acc: 0.7474 - val_loss: 0.4820 - val_precision: 0.7218 - val_recall: 0.7884 - val_acc: 0.7442\n",
            "Epoch 220/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4789 - precision: 0.7394 - recall: 0.7588 - acc: 0.7472 - val_loss: 0.4833 - val_precision: 0.7419 - val_recall: 0.7433 - val_acc: 0.7444\n",
            "Epoch 221/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4793 - precision: 0.7400 - recall: 0.7583 - acc: 0.7475 - val_loss: 0.4877 - val_precision: 0.7087 - val_recall: 0.8185 - val_acc: 0.7430\n",
            "Epoch 222/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4791 - precision: 0.7399 - recall: 0.7585 - acc: 0.7475 - val_loss: 0.4823 - val_precision: 0.7443 - val_recall: 0.7427 - val_acc: 0.7457\n",
            "Epoch 223/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4788 - precision: 0.7407 - recall: 0.7579 - acc: 0.7478 - val_loss: 0.4821 - val_precision: 0.7278 - val_recall: 0.7822 - val_acc: 0.7468\n",
            "Epoch 224/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4791 - precision: 0.7389 - recall: 0.7591 - acc: 0.7470 - val_loss: 0.4799 - val_precision: 0.7386 - val_recall: 0.7498 - val_acc: 0.7442\n",
            "Epoch 225/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4785 - precision: 0.7409 - recall: 0.7581 - acc: 0.7481 - val_loss: 0.4805 - val_precision: 0.7243 - val_recall: 0.7854 - val_acc: 0.7452\n",
            "Epoch 226/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4781 - precision: 0.7406 - recall: 0.7599 - acc: 0.7484 - val_loss: 0.4838 - val_precision: 0.7193 - val_recall: 0.8027 - val_acc: 0.7466\n",
            "Epoch 227/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4784 - precision: 0.7397 - recall: 0.7588 - acc: 0.7475 - val_loss: 0.4804 - val_precision: 0.7231 - val_recall: 0.7941 - val_acc: 0.7469\n",
            "Epoch 228/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4781 - precision: 0.7404 - recall: 0.7597 - acc: 0.7483 - val_loss: 0.4818 - val_precision: 0.7318 - val_recall: 0.7764 - val_acc: 0.7479\n",
            "Epoch 229/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4778 - precision: 0.7406 - recall: 0.7587 - acc: 0.7480 - val_loss: 0.4784 - val_precision: 0.7440 - val_recall: 0.7554 - val_acc: 0.7497\n",
            "Epoch 230/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4777 - precision: 0.7402 - recall: 0.7607 - acc: 0.7484 - val_loss: 0.4793 - val_precision: 0.7320 - val_recall: 0.7714 - val_acc: 0.7464\n",
            "Epoch 231/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4778 - precision: 0.7403 - recall: 0.7600 - acc: 0.7482 - val_loss: 0.4790 - val_precision: 0.7273 - val_recall: 0.7935 - val_acc: 0.7499\n",
            "Epoch 232/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4776 - precision: 0.7405 - recall: 0.7592 - acc: 0.7481 - val_loss: 0.4819 - val_precision: 0.7616 - val_recall: 0.7200 - val_acc: 0.7492\n",
            "Epoch 233/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4777 - precision: 0.7404 - recall: 0.7580 - acc: 0.7477 - val_loss: 0.4778 - val_precision: 0.7452 - val_recall: 0.7440 - val_acc: 0.7468\n",
            "Epoch 234/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4771 - precision: 0.7405 - recall: 0.7593 - acc: 0.7482 - val_loss: 0.4778 - val_precision: 0.7505 - val_recall: 0.7408 - val_acc: 0.7492\n",
            "Epoch 235/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4773 - precision: 0.7412 - recall: 0.7601 - acc: 0.7489 - val_loss: 0.4784 - val_precision: 0.7484 - val_recall: 0.7380 - val_acc: 0.7469\n",
            "Epoch 236/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4770 - precision: 0.7408 - recall: 0.7595 - acc: 0.7484 - val_loss: 0.4815 - val_precision: 0.7256 - val_recall: 0.7790 - val_acc: 0.7441\n",
            "Epoch 237/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4772 - precision: 0.7411 - recall: 0.7591 - acc: 0.7485 - val_loss: 0.4776 - val_precision: 0.7338 - val_recall: 0.7720 - val_acc: 0.7479\n",
            "Epoch 238/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4768 - precision: 0.7419 - recall: 0.7590 - acc: 0.7491 - val_loss: 0.4773 - val_precision: 0.7313 - val_recall: 0.7774 - val_acc: 0.7478\n",
            "Epoch 239/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4766 - precision: 0.7418 - recall: 0.7581 - acc: 0.7487 - val_loss: 0.4776 - val_precision: 0.7409 - val_recall: 0.7538 - val_acc: 0.7470\n",
            "Epoch 240/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4762 - precision: 0.7420 - recall: 0.7601 - acc: 0.7495 - val_loss: 0.4794 - val_precision: 0.7301 - val_recall: 0.7810 - val_acc: 0.7481\n",
            "Epoch 241/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4764 - precision: 0.7419 - recall: 0.7590 - acc: 0.7490 - val_loss: 0.4764 - val_precision: 0.7425 - val_recall: 0.7512 - val_acc: 0.7472\n",
            "Epoch 242/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4763 - precision: 0.7424 - recall: 0.7593 - acc: 0.7494 - val_loss: 0.4769 - val_precision: 0.7364 - val_recall: 0.7756 - val_acc: 0.7509\n",
            "Epoch 243/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4761 - precision: 0.7418 - recall: 0.7599 - acc: 0.7492 - val_loss: 0.4776 - val_precision: 0.7443 - val_recall: 0.7485 - val_acc: 0.7476\n",
            "Epoch 244/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4757 - precision: 0.7413 - recall: 0.7605 - acc: 0.7491 - val_loss: 0.4756 - val_precision: 0.7435 - val_recall: 0.7579 - val_acc: 0.7501\n",
            "Epoch 245/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4753 - precision: 0.7432 - recall: 0.7601 - acc: 0.7502 - val_loss: 0.4793 - val_precision: 0.7214 - val_recall: 0.7953 - val_acc: 0.7460\n",
            "Epoch 246/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4756 - precision: 0.7431 - recall: 0.7595 - acc: 0.7500 - val_loss: 0.4755 - val_precision: 0.7436 - val_recall: 0.7539 - val_acc: 0.7489\n",
            "Epoch 247/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4753 - precision: 0.7430 - recall: 0.7586 - acc: 0.7496 - val_loss: 0.4775 - val_precision: 0.7377 - val_recall: 0.7629 - val_acc: 0.7477\n",
            "Epoch 248/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4756 - precision: 0.7423 - recall: 0.7600 - acc: 0.7496 - val_loss: 0.4768 - val_precision: 0.7442 - val_recall: 0.7549 - val_acc: 0.7496\n",
            "Epoch 249/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4752 - precision: 0.7440 - recall: 0.7582 - acc: 0.7502 - val_loss: 0.4757 - val_precision: 0.7387 - val_recall: 0.7680 - val_acc: 0.7501\n",
            "Epoch 250/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4749 - precision: 0.7431 - recall: 0.7597 - acc: 0.7501 - val_loss: 0.4776 - val_precision: 0.7615 - val_recall: 0.7151 - val_acc: 0.7475\n",
            "Epoch 251/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4750 - precision: 0.7430 - recall: 0.7596 - acc: 0.7500 - val_loss: 0.4769 - val_precision: 0.7472 - val_recall: 0.7453 - val_acc: 0.7485\n",
            "Epoch 252/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4747 - precision: 0.7430 - recall: 0.7588 - acc: 0.7497 - val_loss: 0.4760 - val_precision: 0.7555 - val_recall: 0.7395 - val_acc: 0.7520\n",
            "Epoch 253/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4742 - precision: 0.7434 - recall: 0.7608 - acc: 0.7506 - val_loss: 0.4773 - val_precision: 0.7293 - val_recall: 0.7874 - val_acc: 0.7495\n",
            "Epoch 254/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4743 - precision: 0.7442 - recall: 0.7610 - acc: 0.7513 - val_loss: 0.4752 - val_precision: 0.7570 - val_recall: 0.7325 - val_acc: 0.7506\n",
            "Epoch 255/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4740 - precision: 0.7437 - recall: 0.7601 - acc: 0.7507 - val_loss: 0.4756 - val_precision: 0.7496 - val_recall: 0.7467 - val_acc: 0.7506\n",
            "Epoch 256/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4741 - precision: 0.7427 - recall: 0.7606 - acc: 0.7501 - val_loss: 0.4761 - val_precision: 0.7485 - val_recall: 0.7489 - val_acc: 0.7505\n",
            "Epoch 257/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4736 - precision: 0.7444 - recall: 0.7596 - acc: 0.7510 - val_loss: 0.4750 - val_precision: 0.7492 - val_recall: 0.7529 - val_acc: 0.7523\n",
            "Epoch 258/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4739 - precision: 0.7441 - recall: 0.7592 - acc: 0.7506 - val_loss: 0.4733 - val_precision: 0.7518 - val_recall: 0.7448 - val_acc: 0.7514\n",
            "Epoch 259/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4734 - precision: 0.7446 - recall: 0.7591 - acc: 0.7509 - val_loss: 0.4752 - val_precision: 0.7423 - val_recall: 0.7602 - val_acc: 0.7500\n",
            "Epoch 260/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4733 - precision: 0.7436 - recall: 0.7607 - acc: 0.7507 - val_loss: 0.4746 - val_precision: 0.7465 - val_recall: 0.7561 - val_acc: 0.7516\n",
            "Epoch 261/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4734 - precision: 0.7446 - recall: 0.7599 - acc: 0.7511 - val_loss: 0.4751 - val_precision: 0.7231 - val_recall: 0.7990 - val_acc: 0.7484\n",
            "Epoch 262/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4734 - precision: 0.7437 - recall: 0.7607 - acc: 0.7508 - val_loss: 0.4749 - val_precision: 0.7465 - val_recall: 0.7563 - val_acc: 0.7517\n",
            "Epoch 263/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4725 - precision: 0.7447 - recall: 0.7603 - acc: 0.7514 - val_loss: 0.4738 - val_precision: 0.7418 - val_recall: 0.7595 - val_acc: 0.7495\n",
            "Epoch 264/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4728 - precision: 0.7444 - recall: 0.7609 - acc: 0.7514 - val_loss: 0.4759 - val_precision: 0.7572 - val_recall: 0.7275 - val_acc: 0.7491\n",
            "Epoch 265/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4729 - precision: 0.7445 - recall: 0.7602 - acc: 0.7512 - val_loss: 0.4766 - val_precision: 0.7511 - val_recall: 0.7372 - val_acc: 0.7483\n",
            "Epoch 266/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4726 - precision: 0.7436 - recall: 0.7623 - acc: 0.7513 - val_loss: 0.4748 - val_precision: 0.7409 - val_recall: 0.7675 - val_acc: 0.7515\n",
            "Epoch 267/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4722 - precision: 0.7432 - recall: 0.7621 - acc: 0.7509 - val_loss: 0.4743 - val_precision: 0.7649 - val_recall: 0.7147 - val_acc: 0.7494\n",
            "Epoch 268/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4721 - precision: 0.7444 - recall: 0.7608 - acc: 0.7514 - val_loss: 0.4750 - val_precision: 0.7348 - val_recall: 0.7840 - val_acc: 0.7524\n",
            "Epoch 269/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4719 - precision: 0.7442 - recall: 0.7615 - acc: 0.7514 - val_loss: 0.4754 - val_precision: 0.7364 - val_recall: 0.7722 - val_acc: 0.7498\n",
            "Epoch 270/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4717 - precision: 0.7453 - recall: 0.7631 - acc: 0.7527 - val_loss: 0.4730 - val_precision: 0.7382 - val_recall: 0.7753 - val_acc: 0.7521\n",
            "Epoch 271/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4714 - precision: 0.7445 - recall: 0.7645 - acc: 0.7526 - val_loss: 0.4740 - val_precision: 0.7378 - val_recall: 0.7682 - val_acc: 0.7495\n",
            "Epoch 272/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4720 - precision: 0.7441 - recall: 0.7611 - acc: 0.7512 - val_loss: 0.4745 - val_precision: 0.7426 - val_recall: 0.7592 - val_acc: 0.7500\n",
            "Epoch 273/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4714 - precision: 0.7445 - recall: 0.7614 - acc: 0.7516 - val_loss: 0.4772 - val_precision: 0.7691 - val_recall: 0.6959 - val_acc: 0.7455\n",
            "Epoch 274/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4716 - precision: 0.7446 - recall: 0.7617 - acc: 0.7518 - val_loss: 0.4748 - val_precision: 0.7186 - val_recall: 0.8149 - val_acc: 0.7498\n",
            "Epoch 275/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4716 - precision: 0.7447 - recall: 0.7610 - acc: 0.7516 - val_loss: 0.4746 - val_precision: 0.7226 - val_recall: 0.8074 - val_acc: 0.7506\n",
            "Epoch 276/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4713 - precision: 0.7448 - recall: 0.7634 - acc: 0.7524 - val_loss: 0.4737 - val_precision: 0.7440 - val_recall: 0.7503 - val_acc: 0.7480\n",
            "Epoch 277/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4715 - precision: 0.7437 - recall: 0.7630 - acc: 0.7516 - val_loss: 0.4736 - val_precision: 0.7371 - val_recall: 0.7722 - val_acc: 0.7503\n",
            "Epoch 278/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4711 - precision: 0.7453 - recall: 0.7633 - acc: 0.7528 - val_loss: 0.4733 - val_precision: 0.7360 - val_recall: 0.7736 - val_acc: 0.7500\n",
            "Epoch 279/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4710 - precision: 0.7439 - recall: 0.7648 - acc: 0.7523 - val_loss: 0.4752 - val_precision: 0.7650 - val_recall: 0.7116 - val_acc: 0.7484\n",
            "Epoch 280/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4710 - precision: 0.7448 - recall: 0.7630 - acc: 0.7523 - val_loss: 0.4737 - val_precision: 0.7361 - val_recall: 0.7735 - val_acc: 0.7500\n",
            "Epoch 281/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4709 - precision: 0.7451 - recall: 0.7630 - acc: 0.7525 - val_loss: 0.4728 - val_precision: 0.7318 - val_recall: 0.7858 - val_acc: 0.7508\n",
            "Epoch 282/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4707 - precision: 0.7445 - recall: 0.7649 - acc: 0.7527 - val_loss: 0.4720 - val_precision: 0.7477 - val_recall: 0.7555 - val_acc: 0.7522\n",
            "Epoch 283/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4704 - precision: 0.7447 - recall: 0.7645 - acc: 0.7528 - val_loss: 0.4755 - val_precision: 0.7255 - val_recall: 0.7947 - val_acc: 0.7489\n",
            "Epoch 284/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4703 - precision: 0.7451 - recall: 0.7643 - acc: 0.7529 - val_loss: 0.4734 - val_precision: 0.7569 - val_recall: 0.7373 - val_acc: 0.7522\n",
            "Epoch 285/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4703 - precision: 0.7443 - recall: 0.7655 - acc: 0.7528 - val_loss: 0.4739 - val_precision: 0.7434 - val_recall: 0.7476 - val_acc: 0.7467\n",
            "Epoch 286/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4702 - precision: 0.7450 - recall: 0.7635 - acc: 0.7526 - val_loss: 0.4739 - val_precision: 0.7362 - val_recall: 0.7702 - val_acc: 0.7490\n",
            "Epoch 287/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4700 - precision: 0.7442 - recall: 0.7639 - acc: 0.7522 - val_loss: 0.4722 - val_precision: 0.7342 - val_recall: 0.7788 - val_acc: 0.7503\n",
            "Epoch 288/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4700 - precision: 0.7452 - recall: 0.7640 - acc: 0.7529 - val_loss: 0.4733 - val_precision: 0.7228 - val_recall: 0.8048 - val_acc: 0.7500\n",
            "Epoch 289/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4704 - precision: 0.7445 - recall: 0.7649 - acc: 0.7528 - val_loss: 0.4708 - val_precision: 0.7439 - val_recall: 0.7650 - val_acc: 0.7527\n",
            "Epoch 290/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4702 - precision: 0.7447 - recall: 0.7640 - acc: 0.7526 - val_loss: 0.4711 - val_precision: 0.7365 - val_recall: 0.7846 - val_acc: 0.7538\n",
            "Epoch 291/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4695 - precision: 0.7446 - recall: 0.7658 - acc: 0.7531 - val_loss: 0.4729 - val_precision: 0.7524 - val_recall: 0.7486 - val_acc: 0.7530\n",
            "Epoch 292/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4698 - precision: 0.7446 - recall: 0.7661 - acc: 0.7532 - val_loss: 0.4725 - val_precision: 0.7341 - val_recall: 0.7805 - val_acc: 0.7508\n",
            "Epoch 293/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4698 - precision: 0.7448 - recall: 0.7650 - acc: 0.7530 - val_loss: 0.4729 - val_precision: 0.7240 - val_recall: 0.8051 - val_acc: 0.7510\n",
            "Epoch 294/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4693 - precision: 0.7443 - recall: 0.7663 - acc: 0.7530 - val_loss: 0.4714 - val_precision: 0.7369 - val_recall: 0.7786 - val_acc: 0.7523\n",
            "Epoch 295/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4694 - precision: 0.7446 - recall: 0.7648 - acc: 0.7528 - val_loss: 0.4769 - val_precision: 0.7367 - val_recall: 0.7727 - val_acc: 0.7502\n",
            "Epoch 296/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4698 - precision: 0.7445 - recall: 0.7658 - acc: 0.7530 - val_loss: 0.4715 - val_precision: 0.7432 - val_recall: 0.7618 - val_acc: 0.7512\n",
            "Epoch 297/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4693 - precision: 0.7448 - recall: 0.7662 - acc: 0.7533 - val_loss: 0.4741 - val_precision: 0.7417 - val_recall: 0.7635 - val_acc: 0.7507\n",
            "Epoch 298/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4692 - precision: 0.7436 - recall: 0.7665 - acc: 0.7525 - val_loss: 0.4701 - val_precision: 0.7492 - val_recall: 0.7593 - val_acc: 0.7544\n",
            "Epoch 299/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4693 - precision: 0.7447 - recall: 0.7665 - acc: 0.7534 - val_loss: 0.4699 - val_precision: 0.7321 - val_recall: 0.7917 - val_acc: 0.7529\n",
            "Epoch 300/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4687 - precision: 0.7438 - recall: 0.7672 - acc: 0.7530 - val_loss: 0.4730 - val_precision: 0.7461 - val_recall: 0.7565 - val_acc: 0.7515\n",
            "Epoch 301/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4690 - precision: 0.7435 - recall: 0.7674 - acc: 0.7529 - val_loss: 0.4714 - val_precision: 0.7600 - val_recall: 0.7351 - val_acc: 0.7534\n",
            "Epoch 302/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4687 - precision: 0.7446 - recall: 0.7663 - acc: 0.7532 - val_loss: 0.4703 - val_precision: 0.7533 - val_recall: 0.7461 - val_acc: 0.7528\n",
            "Epoch 303/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4687 - precision: 0.7448 - recall: 0.7665 - acc: 0.7534 - val_loss: 0.4707 - val_precision: 0.7367 - val_recall: 0.7746 - val_acc: 0.7508\n",
            "Epoch 304/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4683 - precision: 0.7442 - recall: 0.7679 - acc: 0.7535 - val_loss: 0.4703 - val_precision: 0.7416 - val_recall: 0.7642 - val_acc: 0.7509\n",
            "Epoch 305/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4686 - precision: 0.7441 - recall: 0.7668 - acc: 0.7531 - val_loss: 0.4694 - val_precision: 0.7528 - val_recall: 0.7495 - val_acc: 0.7536\n",
            "Epoch 306/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4681 - precision: 0.7451 - recall: 0.7680 - acc: 0.7541 - val_loss: 0.4728 - val_precision: 0.7529 - val_recall: 0.7497 - val_acc: 0.7537\n",
            "Epoch 307/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4684 - precision: 0.7447 - recall: 0.7676 - acc: 0.7538 - val_loss: 0.4725 - val_precision: 0.7333 - val_recall: 0.7836 - val_acc: 0.7512\n",
            "Epoch 308/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4684 - precision: 0.7449 - recall: 0.7664 - acc: 0.7535 - val_loss: 0.4723 - val_precision: 0.7616 - val_recall: 0.7291 - val_acc: 0.7523\n",
            "Epoch 309/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4679 - precision: 0.7443 - recall: 0.7678 - acc: 0.7535 - val_loss: 0.4710 - val_precision: 0.7388 - val_recall: 0.7758 - val_acc: 0.7526\n",
            "Epoch 310/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4678 - precision: 0.7446 - recall: 0.7674 - acc: 0.7536 - val_loss: 0.4702 - val_precision: 0.7518 - val_recall: 0.7513 - val_acc: 0.7536\n",
            "Epoch 311/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4680 - precision: 0.7448 - recall: 0.7686 - acc: 0.7542 - val_loss: 0.4719 - val_precision: 0.7498 - val_recall: 0.7495 - val_acc: 0.7516\n",
            "Epoch 312/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4682 - precision: 0.7453 - recall: 0.7672 - acc: 0.7540 - val_loss: 0.4699 - val_precision: 0.7254 - val_recall: 0.8003 - val_acc: 0.7506\n",
            "Epoch 313/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4682 - precision: 0.7442 - recall: 0.7676 - acc: 0.7534 - val_loss: 0.4703 - val_precision: 0.7394 - val_recall: 0.7754 - val_acc: 0.7530\n",
            "Epoch 314/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4681 - precision: 0.7450 - recall: 0.7684 - acc: 0.7542 - val_loss: 0.4711 - val_precision: 0.7371 - val_recall: 0.7795 - val_acc: 0.7527\n",
            "Epoch 315/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4682 - precision: 0.7440 - recall: 0.7675 - acc: 0.7532 - val_loss: 0.4747 - val_precision: 0.7156 - val_recall: 0.8221 - val_acc: 0.7496\n",
            "Epoch 316/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4677 - precision: 0.7449 - recall: 0.7673 - acc: 0.7538 - val_loss: 0.4706 - val_precision: 0.7329 - val_recall: 0.7883 - val_acc: 0.7524\n",
            "Epoch 317/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4675 - precision: 0.7446 - recall: 0.7703 - acc: 0.7546 - val_loss: 0.4692 - val_precision: 0.7389 - val_recall: 0.7717 - val_acc: 0.7514\n",
            "Epoch 318/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4679 - precision: 0.7439 - recall: 0.7697 - acc: 0.7539 - val_loss: 0.4728 - val_precision: 0.7273 - val_recall: 0.7962 - val_acc: 0.7507\n",
            "Epoch 319/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4670 - precision: 0.7449 - recall: 0.7679 - acc: 0.7540 - val_loss: 0.4739 - val_precision: 0.7437 - val_recall: 0.7595 - val_acc: 0.7508\n",
            "Epoch 320/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4674 - precision: 0.7447 - recall: 0.7685 - acc: 0.7540 - val_loss: 0.4720 - val_precision: 0.7690 - val_recall: 0.7183 - val_acc: 0.7532\n",
            "Epoch 321/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4672 - precision: 0.7449 - recall: 0.7682 - acc: 0.7541 - val_loss: 0.4722 - val_precision: 0.7435 - val_recall: 0.7611 - val_acc: 0.7512\n",
            "Epoch 322/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4672 - precision: 0.7446 - recall: 0.7688 - acc: 0.7541 - val_loss: 0.4674 - val_precision: 0.7529 - val_recall: 0.7550 - val_acc: 0.7555\n",
            "Epoch 323/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4668 - precision: 0.7458 - recall: 0.7699 - acc: 0.7552 - val_loss: 0.4692 - val_precision: 0.7561 - val_recall: 0.7490 - val_acc: 0.7556\n",
            "Epoch 324/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4670 - precision: 0.7446 - recall: 0.7703 - acc: 0.7546 - val_loss: 0.4702 - val_precision: 0.7355 - val_recall: 0.7768 - val_acc: 0.7507\n",
            "Epoch 325/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4671 - precision: 0.7454 - recall: 0.7695 - acc: 0.7548 - val_loss: 0.4705 - val_precision: 0.7322 - val_recall: 0.7834 - val_acc: 0.7506\n",
            "Epoch 326/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4672 - precision: 0.7442 - recall: 0.7684 - acc: 0.7537 - val_loss: 0.4717 - val_precision: 0.7729 - val_recall: 0.7026 - val_acc: 0.7500\n",
            "Epoch 327/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4672 - precision: 0.7448 - recall: 0.7703 - acc: 0.7547 - val_loss: 0.4702 - val_precision: 0.7447 - val_recall: 0.7626 - val_acc: 0.7525\n",
            "Epoch 328/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4670 - precision: 0.7443 - recall: 0.7689 - acc: 0.7539 - val_loss: 0.4678 - val_precision: 0.7535 - val_recall: 0.7521 - val_acc: 0.7549\n",
            "Epoch 329/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4666 - precision: 0.7445 - recall: 0.7700 - acc: 0.7544 - val_loss: 0.4714 - val_precision: 0.7730 - val_recall: 0.7096 - val_acc: 0.7525\n",
            "Epoch 330/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4666 - precision: 0.7453 - recall: 0.7680 - acc: 0.7543 - val_loss: 0.4708 - val_precision: 0.7519 - val_recall: 0.7477 - val_acc: 0.7524\n",
            "Epoch 331/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4663 - precision: 0.7449 - recall: 0.7700 - acc: 0.7547 - val_loss: 0.4680 - val_precision: 0.7653 - val_recall: 0.7353 - val_acc: 0.7567\n",
            "Epoch 332/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4663 - precision: 0.7455 - recall: 0.7701 - acc: 0.7551 - val_loss: 0.4669 - val_precision: 0.7493 - val_recall: 0.7634 - val_acc: 0.7559\n",
            "Epoch 333/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4663 - precision: 0.7448 - recall: 0.7691 - acc: 0.7543 - val_loss: 0.4709 - val_precision: 0.7590 - val_recall: 0.7332 - val_acc: 0.7521\n",
            "Epoch 334/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4667 - precision: 0.7456 - recall: 0.7697 - acc: 0.7551 - val_loss: 0.4683 - val_precision: 0.7415 - val_recall: 0.7778 - val_acc: 0.7552\n",
            "Epoch 335/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4664 - precision: 0.7450 - recall: 0.7709 - acc: 0.7550 - val_loss: 0.4697 - val_precision: 0.7296 - val_recall: 0.7974 - val_acc: 0.7528\n",
            "Epoch 336/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4660 - precision: 0.7448 - recall: 0.7705 - acc: 0.7548 - val_loss: 0.4683 - val_precision: 0.7409 - val_recall: 0.7780 - val_acc: 0.7549\n",
            "Epoch 337/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4660 - precision: 0.7454 - recall: 0.7707 - acc: 0.7552 - val_loss: 0.4695 - val_precision: 0.7342 - val_recall: 0.7901 - val_acc: 0.7539\n",
            "Epoch 338/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4667 - precision: 0.7438 - recall: 0.7706 - acc: 0.7541 - val_loss: 0.4689 - val_precision: 0.7361 - val_recall: 0.7824 - val_acc: 0.7528\n",
            "Epoch 339/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4660 - precision: 0.7454 - recall: 0.7705 - acc: 0.7552 - val_loss: 0.4683 - val_precision: 0.7565 - val_recall: 0.7323 - val_acc: 0.7502\n",
            "Epoch 340/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4659 - precision: 0.7452 - recall: 0.7699 - acc: 0.7549 - val_loss: 0.4667 - val_precision: 0.7376 - val_recall: 0.7799 - val_acc: 0.7531\n",
            "Epoch 341/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4660 - precision: 0.7444 - recall: 0.7705 - acc: 0.7545 - val_loss: 0.4693 - val_precision: 0.7266 - val_recall: 0.7981 - val_acc: 0.7508\n",
            "Epoch 342/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4660 - precision: 0.7456 - recall: 0.7722 - acc: 0.7559 - val_loss: 0.4687 - val_precision: 0.7375 - val_recall: 0.7789 - val_acc: 0.7527\n",
            "Epoch 343/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4657 - precision: 0.7444 - recall: 0.7707 - acc: 0.7545 - val_loss: 0.4698 - val_precision: 0.7548 - val_recall: 0.7430 - val_acc: 0.7527\n",
            "Epoch 344/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4657 - precision: 0.7457 - recall: 0.7701 - acc: 0.7553 - val_loss: 0.4686 - val_precision: 0.7363 - val_recall: 0.7785 - val_acc: 0.7518\n",
            "Epoch 345/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4656 - precision: 0.7445 - recall: 0.7716 - acc: 0.7550 - val_loss: 0.4697 - val_precision: 0.7657 - val_recall: 0.7221 - val_acc: 0.7525\n",
            "Epoch 346/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4657 - precision: 0.7459 - recall: 0.7691 - acc: 0.7550 - val_loss: 0.4684 - val_precision: 0.7500 - val_recall: 0.7561 - val_acc: 0.7539\n",
            "Epoch 347/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4653 - precision: 0.7448 - recall: 0.7704 - acc: 0.7547 - val_loss: 0.4663 - val_precision: 0.7457 - val_recall: 0.7685 - val_acc: 0.7551\n",
            "Epoch 348/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4654 - precision: 0.7457 - recall: 0.7713 - acc: 0.7556 - val_loss: 0.4684 - val_precision: 0.7385 - val_recall: 0.7803 - val_acc: 0.7539\n",
            "Epoch 349/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4653 - precision: 0.7453 - recall: 0.7703 - acc: 0.7551 - val_loss: 0.4705 - val_precision: 0.7447 - val_recall: 0.7634 - val_acc: 0.7527\n",
            "Epoch 350/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4650 - precision: 0.7447 - recall: 0.7713 - acc: 0.7550 - val_loss: 0.4655 - val_precision: 0.7488 - val_recall: 0.7593 - val_acc: 0.7542\n",
            "Epoch 351/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4650 - precision: 0.7458 - recall: 0.7698 - acc: 0.7552 - val_loss: 0.4689 - val_precision: 0.7571 - val_recall: 0.7365 - val_acc: 0.7520\n",
            "Epoch 352/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4652 - precision: 0.7445 - recall: 0.7709 - acc: 0.7547 - val_loss: 0.4677 - val_precision: 0.7302 - val_recall: 0.7904 - val_acc: 0.7511\n",
            "Epoch 353/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4651 - precision: 0.7450 - recall: 0.7698 - acc: 0.7547 - val_loss: 0.4673 - val_precision: 0.7406 - val_recall: 0.7875 - val_acc: 0.7577\n",
            "Epoch 354/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4649 - precision: 0.7442 - recall: 0.7718 - acc: 0.7548 - val_loss: 0.4696 - val_precision: 0.7506 - val_recall: 0.7498 - val_acc: 0.7522\n",
            "Epoch 355/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4651 - precision: 0.7445 - recall: 0.7723 - acc: 0.7552 - val_loss: 0.4653 - val_precision: 0.7357 - val_recall: 0.7874 - val_acc: 0.7541\n",
            "Epoch 356/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4645 - precision: 0.7448 - recall: 0.7717 - acc: 0.7552 - val_loss: 0.4655 - val_precision: 0.7405 - val_recall: 0.7793 - val_acc: 0.7550\n",
            "Epoch 357/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4644 - precision: 0.7463 - recall: 0.7707 - acc: 0.7559 - val_loss: 0.4650 - val_precision: 0.7476 - val_recall: 0.7637 - val_acc: 0.7548\n",
            "Epoch 358/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4646 - precision: 0.7444 - recall: 0.7715 - acc: 0.7548 - val_loss: 0.4704 - val_precision: 0.7644 - val_recall: 0.7208 - val_acc: 0.7513\n",
            "Epoch 359/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4645 - precision: 0.7449 - recall: 0.7730 - acc: 0.7556 - val_loss: 0.4662 - val_precision: 0.7529 - val_recall: 0.7501 - val_acc: 0.7538\n",
            "Epoch 360/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4647 - precision: 0.7464 - recall: 0.7698 - acc: 0.7556 - val_loss: 0.4672 - val_precision: 0.7451 - val_recall: 0.7654 - val_acc: 0.7537\n",
            "Epoch 361/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4644 - precision: 0.7457 - recall: 0.7705 - acc: 0.7554 - val_loss: 0.4671 - val_precision: 0.7375 - val_recall: 0.7875 - val_acc: 0.7555\n",
            "Epoch 362/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4644 - precision: 0.7452 - recall: 0.7713 - acc: 0.7553 - val_loss: 0.4650 - val_precision: 0.7401 - val_recall: 0.7893 - val_acc: 0.7580\n",
            "Epoch 363/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4644 - precision: 0.7462 - recall: 0.7698 - acc: 0.7555 - val_loss: 0.4675 - val_precision: 0.7452 - val_recall: 0.7726 - val_acc: 0.7561\n",
            "Epoch 364/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4645 - precision: 0.7444 - recall: 0.7729 - acc: 0.7553 - val_loss: 0.4653 - val_precision: 0.7461 - val_recall: 0.7665 - val_acc: 0.7547\n",
            "Epoch 365/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4644 - precision: 0.7460 - recall: 0.7706 - acc: 0.7556 - val_loss: 0.4661 - val_precision: 0.7405 - val_recall: 0.7752 - val_acc: 0.7537\n",
            "Epoch 366/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4642 - precision: 0.7453 - recall: 0.7714 - acc: 0.7554 - val_loss: 0.4662 - val_precision: 0.7423 - val_recall: 0.7806 - val_acc: 0.7567\n",
            "Epoch 367/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4643 - precision: 0.7446 - recall: 0.7705 - acc: 0.7546 - val_loss: 0.4668 - val_precision: 0.7476 - val_recall: 0.7640 - val_acc: 0.7549\n",
            "Epoch 368/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4641 - precision: 0.7455 - recall: 0.7709 - acc: 0.7554 - val_loss: 0.4648 - val_precision: 0.7314 - val_recall: 0.7943 - val_acc: 0.7532\n",
            "Epoch 369/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4639 - precision: 0.7451 - recall: 0.7719 - acc: 0.7554 - val_loss: 0.4678 - val_precision: 0.7669 - val_recall: 0.7191 - val_acc: 0.7522\n",
            "Epoch 370/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4640 - precision: 0.7450 - recall: 0.7709 - acc: 0.7550 - val_loss: 0.4663 - val_precision: 0.7699 - val_recall: 0.7195 - val_acc: 0.7541\n",
            "Epoch 371/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4642 - precision: 0.7462 - recall: 0.7714 - acc: 0.7560 - val_loss: 0.4676 - val_precision: 0.7360 - val_recall: 0.7862 - val_acc: 0.7539\n",
            "Epoch 372/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4639 - precision: 0.7456 - recall: 0.7715 - acc: 0.7556 - val_loss: 0.4674 - val_precision: 0.7344 - val_recall: 0.7890 - val_acc: 0.7539\n",
            "Epoch 373/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4638 - precision: 0.7453 - recall: 0.7717 - acc: 0.7555 - val_loss: 0.4683 - val_precision: 0.7622 - val_recall: 0.7265 - val_acc: 0.7518\n",
            "Epoch 374/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4639 - precision: 0.7451 - recall: 0.7707 - acc: 0.7551 - val_loss: 0.4683 - val_precision: 0.7566 - val_recall: 0.7394 - val_acc: 0.7527\n",
            "Epoch 375/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4641 - precision: 0.7452 - recall: 0.7722 - acc: 0.7556 - val_loss: 0.4709 - val_precision: 0.7333 - val_recall: 0.7946 - val_acc: 0.7547\n",
            "Epoch 376/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4639 - precision: 0.7456 - recall: 0.7729 - acc: 0.7561 - val_loss: 0.4642 - val_precision: 0.7517 - val_recall: 0.7578 - val_acc: 0.7556\n",
            "Epoch 377/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4635 - precision: 0.7455 - recall: 0.7734 - acc: 0.7562 - val_loss: 0.4656 - val_precision: 0.7392 - val_recall: 0.7903 - val_acc: 0.7576\n",
            "Epoch 378/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4634 - precision: 0.7463 - recall: 0.7713 - acc: 0.7561 - val_loss: 0.4653 - val_precision: 0.7415 - val_recall: 0.7748 - val_acc: 0.7542\n",
            "Epoch 379/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4638 - precision: 0.7454 - recall: 0.7708 - acc: 0.7553 - val_loss: 0.4650 - val_precision: 0.7333 - val_recall: 0.7997 - val_acc: 0.7563\n",
            "Epoch 380/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4640 - precision: 0.7445 - recall: 0.7733 - acc: 0.7554 - val_loss: 0.4668 - val_precision: 0.7664 - val_recall: 0.7270 - val_acc: 0.7546\n",
            "Epoch 381/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4633 - precision: 0.7466 - recall: 0.7706 - acc: 0.7561 - val_loss: 0.4653 - val_precision: 0.7495 - val_recall: 0.7679 - val_acc: 0.7575\n",
            "Epoch 382/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4635 - precision: 0.7460 - recall: 0.7723 - acc: 0.7562 - val_loss: 0.4675 - val_precision: 0.7597 - val_recall: 0.7388 - val_acc: 0.7544\n",
            "Epoch 383/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4634 - precision: 0.7459 - recall: 0.7727 - acc: 0.7562 - val_loss: 0.4680 - val_precision: 0.7303 - val_recall: 0.7895 - val_acc: 0.7509\n",
            "Epoch 384/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4630 - precision: 0.7450 - recall: 0.7732 - acc: 0.7558 - val_loss: 0.4660 - val_precision: 0.7477 - val_recall: 0.7560 - val_acc: 0.7524\n",
            "Epoch 385/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4630 - precision: 0.7453 - recall: 0.7718 - acc: 0.7556 - val_loss: 0.4672 - val_precision: 0.7717 - val_recall: 0.7157 - val_acc: 0.7539\n",
            "Epoch 386/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4628 - precision: 0.7447 - recall: 0.7729 - acc: 0.7554 - val_loss: 0.4639 - val_precision: 0.7513 - val_recall: 0.7576 - val_acc: 0.7553\n",
            "Epoch 387/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4633 - precision: 0.7448 - recall: 0.7733 - acc: 0.7557 - val_loss: 0.4667 - val_precision: 0.7646 - val_recall: 0.7293 - val_acc: 0.7543\n",
            "Epoch 388/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4632 - precision: 0.7449 - recall: 0.7724 - acc: 0.7555 - val_loss: 0.4639 - val_precision: 0.7356 - val_recall: 0.7903 - val_acc: 0.7550\n",
            "Epoch 389/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4628 - precision: 0.7448 - recall: 0.7734 - acc: 0.7557 - val_loss: 0.4672 - val_precision: 0.7389 - val_recall: 0.7759 - val_acc: 0.7528\n",
            "Epoch 390/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4627 - precision: 0.7455 - recall: 0.7734 - acc: 0.7562 - val_loss: 0.4651 - val_precision: 0.7453 - val_recall: 0.7594 - val_acc: 0.7518\n",
            "Epoch 391/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4627 - precision: 0.7458 - recall: 0.7734 - acc: 0.7564 - val_loss: 0.4663 - val_precision: 0.7472 - val_recall: 0.7635 - val_acc: 0.7544\n",
            "Epoch 392/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4628 - precision: 0.7457 - recall: 0.7722 - acc: 0.7559 - val_loss: 0.4647 - val_precision: 0.7403 - val_recall: 0.7842 - val_acc: 0.7564\n",
            "Epoch 393/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4624 - precision: 0.7454 - recall: 0.7745 - acc: 0.7565 - val_loss: 0.4661 - val_precision: 0.7451 - val_recall: 0.7686 - val_acc: 0.7547\n",
            "Epoch 394/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4625 - precision: 0.7453 - recall: 0.7731 - acc: 0.7560 - val_loss: 0.4692 - val_precision: 0.7285 - val_recall: 0.8028 - val_acc: 0.7537\n",
            "Epoch 395/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4629 - precision: 0.7448 - recall: 0.7735 - acc: 0.7557 - val_loss: 0.4626 - val_precision: 0.7611 - val_recall: 0.7413 - val_acc: 0.7562\n",
            "Epoch 396/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4626 - precision: 0.7457 - recall: 0.7725 - acc: 0.7560 - val_loss: 0.4669 - val_precision: 0.7215 - val_recall: 0.8256 - val_acc: 0.7553\n",
            "Epoch 397/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4622 - precision: 0.7449 - recall: 0.7748 - acc: 0.7562 - val_loss: 0.4627 - val_precision: 0.7563 - val_recall: 0.7497 - val_acc: 0.7559\n",
            "Epoch 398/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4627 - precision: 0.7439 - recall: 0.7746 - acc: 0.7555 - val_loss: 0.4647 - val_precision: 0.7620 - val_recall: 0.7414 - val_acc: 0.7568\n",
            "Epoch 399/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4629 - precision: 0.7439 - recall: 0.7734 - acc: 0.7551 - val_loss: 0.4636 - val_precision: 0.7602 - val_recall: 0.7407 - val_acc: 0.7554\n",
            "Epoch 400/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4622 - precision: 0.7455 - recall: 0.7729 - acc: 0.7560 - val_loss: 0.4638 - val_precision: 0.7339 - val_recall: 0.7831 - val_acc: 0.7515\n",
            "Epoch 401/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4620 - precision: 0.7460 - recall: 0.7752 - acc: 0.7571 - val_loss: 0.4643 - val_precision: 0.7473 - val_recall: 0.7643 - val_acc: 0.7548\n",
            "Epoch 402/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4619 - precision: 0.7453 - recall: 0.7749 - acc: 0.7565 - val_loss: 0.4639 - val_precision: 0.7451 - val_recall: 0.7703 - val_acc: 0.7553\n",
            "Epoch 403/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4620 - precision: 0.7455 - recall: 0.7753 - acc: 0.7568 - val_loss: 0.4639 - val_precision: 0.7334 - val_recall: 0.7984 - val_acc: 0.7560\n",
            "Epoch 404/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4620 - precision: 0.7450 - recall: 0.7758 - acc: 0.7567 - val_loss: 0.4632 - val_precision: 0.7418 - val_recall: 0.7762 - val_acc: 0.7549\n",
            "Epoch 405/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4622 - precision: 0.7445 - recall: 0.7771 - acc: 0.7567 - val_loss: 0.4625 - val_precision: 0.7477 - val_recall: 0.7692 - val_acc: 0.7567\n",
            "Epoch 406/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4619 - precision: 0.7447 - recall: 0.7745 - acc: 0.7560 - val_loss: 0.4636 - val_precision: 0.7327 - val_recall: 0.7990 - val_acc: 0.7557\n",
            "Epoch 407/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4619 - precision: 0.7440 - recall: 0.7766 - acc: 0.7562 - val_loss: 0.4678 - val_precision: 0.7656 - val_recall: 0.7236 - val_acc: 0.7529\n",
            "Epoch 408/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4615 - precision: 0.7447 - recall: 0.7754 - acc: 0.7563 - val_loss: 0.4652 - val_precision: 0.7585 - val_recall: 0.7401 - val_acc: 0.7541\n",
            "Epoch 409/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4616 - precision: 0.7463 - recall: 0.7746 - acc: 0.7571 - val_loss: 0.4623 - val_precision: 0.7384 - val_recall: 0.7789 - val_acc: 0.7534\n",
            "Epoch 410/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4617 - precision: 0.7443 - recall: 0.7773 - acc: 0.7566 - val_loss: 0.4616 - val_precision: 0.7449 - val_recall: 0.7760 - val_acc: 0.7570\n",
            "Epoch 411/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4618 - precision: 0.7443 - recall: 0.7755 - acc: 0.7561 - val_loss: 0.4643 - val_precision: 0.7687 - val_recall: 0.7243 - val_acc: 0.7551\n",
            "Epoch 412/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4617 - precision: 0.7439 - recall: 0.7779 - acc: 0.7565 - val_loss: 0.4637 - val_precision: 0.7629 - val_recall: 0.7442 - val_acc: 0.7583\n",
            "Epoch 413/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4613 - precision: 0.7451 - recall: 0.7752 - acc: 0.7565 - val_loss: 0.4651 - val_precision: 0.7387 - val_recall: 0.7742 - val_acc: 0.7521\n",
            "Epoch 414/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4618 - precision: 0.7439 - recall: 0.7772 - acc: 0.7564 - val_loss: 0.4642 - val_precision: 0.7552 - val_recall: 0.7523 - val_acc: 0.7561\n",
            "Epoch 415/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4615 - precision: 0.7455 - recall: 0.7758 - acc: 0.7570 - val_loss: 0.4639 - val_precision: 0.7606 - val_recall: 0.7433 - val_acc: 0.7565\n",
            "Epoch 416/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4612 - precision: 0.7454 - recall: 0.7769 - acc: 0.7572 - val_loss: 0.4638 - val_precision: 0.7443 - val_recall: 0.7660 - val_acc: 0.7533\n",
            "Epoch 417/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4614 - precision: 0.7443 - recall: 0.7789 - acc: 0.7572 - val_loss: 0.4651 - val_precision: 0.7612 - val_recall: 0.7273 - val_acc: 0.7515\n",
            "Epoch 418/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4612 - precision: 0.7445 - recall: 0.7765 - acc: 0.7565 - val_loss: 0.4646 - val_precision: 0.7448 - val_recall: 0.7683 - val_acc: 0.7544\n",
            "Epoch 419/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4612 - precision: 0.7463 - recall: 0.7767 - acc: 0.7578 - val_loss: 0.4615 - val_precision: 0.7472 - val_recall: 0.7707 - val_acc: 0.7569\n",
            "Epoch 420/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4608 - precision: 0.7453 - recall: 0.7781 - acc: 0.7576 - val_loss: 0.4626 - val_precision: 0.7457 - val_recall: 0.7709 - val_acc: 0.7558\n",
            "Epoch 421/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4610 - precision: 0.7452 - recall: 0.7769 - acc: 0.7571 - val_loss: 0.4625 - val_precision: 0.7637 - val_recall: 0.7361 - val_acc: 0.7560\n",
            "Epoch 422/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4609 - precision: 0.7446 - recall: 0.7776 - acc: 0.7569 - val_loss: 0.4629 - val_precision: 0.7392 - val_recall: 0.7806 - val_acc: 0.7545\n",
            "Epoch 423/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4616 - precision: 0.7439 - recall: 0.7777 - acc: 0.7565 - val_loss: 0.4629 - val_precision: 0.7389 - val_recall: 0.7837 - val_acc: 0.7552\n",
            "Epoch 424/500\n",
            "283028/283028 [==============================] - 3s 10us/step - loss: 0.4613 - precision: 0.7438 - recall: 0.7774 - acc: 0.7563 - val_loss: 0.4632 - val_precision: 0.7388 - val_recall: 0.7990 - val_acc: 0.7601\n",
            "Epoch 425/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4611 - precision: 0.7448 - recall: 0.7763 - acc: 0.7567 - val_loss: 0.4659 - val_precision: 0.7181 - val_recall: 0.8325 - val_acc: 0.7548\n",
            "Epoch 426/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4616 - precision: 0.7446 - recall: 0.7767 - acc: 0.7566 - val_loss: 0.4623 - val_precision: 0.7287 - val_recall: 0.8110 - val_acc: 0.7564\n",
            "Epoch 427/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4609 - precision: 0.7443 - recall: 0.7776 - acc: 0.7568 - val_loss: 0.4615 - val_precision: 0.7428 - val_recall: 0.7834 - val_acc: 0.7579\n",
            "Epoch 428/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4609 - precision: 0.7452 - recall: 0.7759 - acc: 0.7568 - val_loss: 0.4630 - val_precision: 0.7477 - val_recall: 0.7632 - val_acc: 0.7547\n",
            "Epoch 429/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4605 - precision: 0.7451 - recall: 0.7784 - acc: 0.7575 - val_loss: 0.4620 - val_precision: 0.7479 - val_recall: 0.7642 - val_acc: 0.7552\n",
            "Epoch 430/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4611 - precision: 0.7438 - recall: 0.7783 - acc: 0.7567 - val_loss: 0.4652 - val_precision: 0.7286 - val_recall: 0.8143 - val_acc: 0.7574\n",
            "Epoch 431/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4606 - precision: 0.7454 - recall: 0.7779 - acc: 0.7576 - val_loss: 0.4628 - val_precision: 0.7396 - val_recall: 0.7886 - val_acc: 0.7574\n",
            "Epoch 432/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4605 - precision: 0.7455 - recall: 0.7779 - acc: 0.7577 - val_loss: 0.4634 - val_precision: 0.7396 - val_recall: 0.7904 - val_acc: 0.7579\n",
            "Epoch 433/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4602 - precision: 0.7455 - recall: 0.7767 - acc: 0.7573 - val_loss: 0.4604 - val_precision: 0.7529 - val_recall: 0.7603 - val_acc: 0.7573\n",
            "Epoch 434/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4607 - precision: 0.7448 - recall: 0.7787 - acc: 0.7575 - val_loss: 0.4613 - val_precision: 0.7452 - val_recall: 0.7713 - val_acc: 0.7556\n",
            "Epoch 435/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4607 - precision: 0.7452 - recall: 0.7772 - acc: 0.7572 - val_loss: 0.4608 - val_precision: 0.7473 - val_recall: 0.7756 - val_acc: 0.7585\n",
            "Epoch 436/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4601 - precision: 0.7457 - recall: 0.7776 - acc: 0.7577 - val_loss: 0.4626 - val_precision: 0.7324 - val_recall: 0.7963 - val_acc: 0.7546\n",
            "Epoch 437/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4602 - precision: 0.7452 - recall: 0.7777 - acc: 0.7574 - val_loss: 0.4624 - val_precision: 0.7553 - val_recall: 0.7562 - val_acc: 0.7575\n",
            "Epoch 438/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4608 - precision: 0.7443 - recall: 0.7769 - acc: 0.7565 - val_loss: 0.4624 - val_precision: 0.7322 - val_recall: 0.8078 - val_acc: 0.7580\n",
            "Epoch 439/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4606 - precision: 0.7451 - recall: 0.7767 - acc: 0.7570 - val_loss: 0.4624 - val_precision: 0.7418 - val_recall: 0.7701 - val_acc: 0.7529\n",
            "Epoch 440/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4603 - precision: 0.7454 - recall: 0.7766 - acc: 0.7572 - val_loss: 0.4615 - val_precision: 0.7332 - val_recall: 0.7976 - val_acc: 0.7555\n",
            "Epoch 441/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4604 - precision: 0.7452 - recall: 0.7773 - acc: 0.7573 - val_loss: 0.4615 - val_precision: 0.7437 - val_recall: 0.7707 - val_acc: 0.7544\n",
            "Epoch 442/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4603 - precision: 0.7452 - recall: 0.7767 - acc: 0.7571 - val_loss: 0.4639 - val_precision: 0.7355 - val_recall: 0.7932 - val_acc: 0.7558\n",
            "Epoch 443/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4602 - precision: 0.7457 - recall: 0.7775 - acc: 0.7577 - val_loss: 0.4617 - val_precision: 0.7467 - val_recall: 0.7809 - val_acc: 0.7598\n",
            "Epoch 444/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4602 - precision: 0.7457 - recall: 0.7776 - acc: 0.7577 - val_loss: 0.4598 - val_precision: 0.7426 - val_recall: 0.7846 - val_acc: 0.7581\n",
            "Epoch 445/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4601 - precision: 0.7451 - recall: 0.7781 - acc: 0.7574 - val_loss: 0.4609 - val_precision: 0.7417 - val_recall: 0.7874 - val_acc: 0.7585\n",
            "Epoch 446/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4598 - precision: 0.7456 - recall: 0.7778 - acc: 0.7577 - val_loss: 0.4639 - val_precision: 0.7633 - val_recall: 0.7352 - val_acc: 0.7555\n",
            "Epoch 447/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4597 - precision: 0.7461 - recall: 0.7775 - acc: 0.7580 - val_loss: 0.4634 - val_precision: 0.7603 - val_recall: 0.7377 - val_acc: 0.7545\n",
            "Epoch 448/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4598 - precision: 0.7455 - recall: 0.7770 - acc: 0.7574 - val_loss: 0.4631 - val_precision: 0.7274 - val_recall: 0.8071 - val_acc: 0.7542\n",
            "Epoch 449/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4600 - precision: 0.7442 - recall: 0.7793 - acc: 0.7572 - val_loss: 0.4675 - val_precision: 0.7455 - val_recall: 0.7732 - val_acc: 0.7565\n",
            "Epoch 450/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4596 - precision: 0.7456 - recall: 0.7784 - acc: 0.7579 - val_loss: 0.4630 - val_precision: 0.7564 - val_recall: 0.7469 - val_acc: 0.7550\n",
            "Epoch 451/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4597 - precision: 0.7464 - recall: 0.7774 - acc: 0.7581 - val_loss: 0.4643 - val_precision: 0.7681 - val_recall: 0.7271 - val_acc: 0.7557\n",
            "Epoch 452/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4597 - precision: 0.7463 - recall: 0.7776 - acc: 0.7581 - val_loss: 0.4596 - val_precision: 0.7444 - val_recall: 0.7905 - val_acc: 0.7614\n",
            "Epoch 453/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4594 - precision: 0.7473 - recall: 0.7778 - acc: 0.7589 - val_loss: 0.4602 - val_precision: 0.7355 - val_recall: 0.7982 - val_acc: 0.7574\n",
            "Epoch 454/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4598 - precision: 0.7456 - recall: 0.7780 - acc: 0.7578 - val_loss: 0.4630 - val_precision: 0.7512 - val_recall: 0.7657 - val_acc: 0.7579\n",
            "Epoch 455/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4596 - precision: 0.7462 - recall: 0.7785 - acc: 0.7583 - val_loss: 0.4616 - val_precision: 0.7287 - val_recall: 0.8094 - val_acc: 0.7559\n",
            "Epoch 456/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4594 - precision: 0.7462 - recall: 0.7783 - acc: 0.7583 - val_loss: 0.4629 - val_precision: 0.7349 - val_recall: 0.7921 - val_acc: 0.7551\n",
            "Epoch 457/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4595 - precision: 0.7452 - recall: 0.7804 - acc: 0.7583 - val_loss: 0.4623 - val_precision: 0.7584 - val_recall: 0.7426 - val_acc: 0.7549\n",
            "Epoch 458/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4600 - precision: 0.7450 - recall: 0.7788 - acc: 0.7576 - val_loss: 0.4607 - val_precision: 0.7424 - val_recall: 0.7885 - val_acc: 0.7593\n",
            "Epoch 459/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4597 - precision: 0.7454 - recall: 0.7776 - acc: 0.7575 - val_loss: 0.4682 - val_precision: 0.7643 - val_recall: 0.7240 - val_acc: 0.7523\n",
            "Epoch 460/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4593 - precision: 0.7450 - recall: 0.7788 - acc: 0.7576 - val_loss: 0.4576 - val_precision: 0.7395 - val_recall: 0.7947 - val_acc: 0.7592\n",
            "Epoch 461/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4598 - precision: 0.7458 - recall: 0.7790 - acc: 0.7582 - val_loss: 0.4598 - val_precision: 0.7388 - val_recall: 0.7913 - val_acc: 0.7576\n",
            "Epoch 462/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4600 - precision: 0.7462 - recall: 0.7771 - acc: 0.7579 - val_loss: 0.4637 - val_precision: 0.7330 - val_recall: 0.8047 - val_acc: 0.7576\n",
            "Epoch 463/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4590 - precision: 0.7454 - recall: 0.7802 - acc: 0.7583 - val_loss: 0.4668 - val_precision: 0.7530 - val_recall: 0.7501 - val_acc: 0.7539\n",
            "Epoch 464/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4592 - precision: 0.7450 - recall: 0.7789 - acc: 0.7576 - val_loss: 0.4610 - val_precision: 0.7400 - val_recall: 0.7890 - val_acc: 0.7577\n",
            "Epoch 465/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4593 - precision: 0.7462 - recall: 0.7769 - acc: 0.7578 - val_loss: 0.4621 - val_precision: 0.7445 - val_recall: 0.7722 - val_acc: 0.7555\n",
            "Epoch 466/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4593 - precision: 0.7455 - recall: 0.7784 - acc: 0.7579 - val_loss: 0.4617 - val_precision: 0.7725 - val_recall: 0.7235 - val_acc: 0.7571\n",
            "Epoch 467/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4590 - precision: 0.7464 - recall: 0.7775 - acc: 0.7582 - val_loss: 0.4606 - val_precision: 0.7521 - val_recall: 0.7562 - val_acc: 0.7554\n",
            "Epoch 468/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4591 - precision: 0.7454 - recall: 0.7788 - acc: 0.7579 - val_loss: 0.4609 - val_precision: 0.7514 - val_recall: 0.7636 - val_acc: 0.7574\n",
            "Epoch 469/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4590 - precision: 0.7460 - recall: 0.7775 - acc: 0.7579 - val_loss: 0.4616 - val_precision: 0.7289 - val_recall: 0.8140 - val_acc: 0.7575\n",
            "Epoch 470/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4590 - precision: 0.7448 - recall: 0.7795 - acc: 0.7577 - val_loss: 0.4635 - val_precision: 0.7651 - val_recall: 0.7261 - val_acc: 0.7535\n",
            "Epoch 471/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4592 - precision: 0.7464 - recall: 0.7783 - acc: 0.7584 - val_loss: 0.4621 - val_precision: 0.7692 - val_recall: 0.7262 - val_acc: 0.7560\n",
            "Epoch 472/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4591 - precision: 0.7458 - recall: 0.7785 - acc: 0.7580 - val_loss: 0.4612 - val_precision: 0.7416 - val_recall: 0.7899 - val_acc: 0.7592\n",
            "Epoch 473/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4591 - precision: 0.7448 - recall: 0.7800 - acc: 0.7579 - val_loss: 0.4589 - val_precision: 0.7532 - val_recall: 0.7616 - val_acc: 0.7579\n",
            "Epoch 474/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4590 - precision: 0.7454 - recall: 0.7798 - acc: 0.7582 - val_loss: 0.4614 - val_precision: 0.7432 - val_recall: 0.7766 - val_acc: 0.7560\n",
            "Epoch 475/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4590 - precision: 0.7455 - recall: 0.7803 - acc: 0.7584 - val_loss: 0.4607 - val_precision: 0.7478 - val_recall: 0.7670 - val_acc: 0.7560\n",
            "Epoch 476/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4587 - precision: 0.7458 - recall: 0.7783 - acc: 0.7580 - val_loss: 0.4623 - val_precision: 0.7345 - val_recall: 0.7999 - val_acc: 0.7573\n",
            "Epoch 477/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4589 - precision: 0.7465 - recall: 0.7773 - acc: 0.7582 - val_loss: 0.4596 - val_precision: 0.7472 - val_recall: 0.7759 - val_acc: 0.7586\n",
            "Epoch 478/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4593 - precision: 0.7449 - recall: 0.7783 - acc: 0.7574 - val_loss: 0.4612 - val_precision: 0.7299 - val_recall: 0.8081 - val_acc: 0.7564\n",
            "Epoch 479/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4588 - precision: 0.7452 - recall: 0.7788 - acc: 0.7578 - val_loss: 0.4615 - val_precision: 0.7421 - val_recall: 0.7729 - val_acc: 0.7540\n",
            "Epoch 480/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4587 - precision: 0.7456 - recall: 0.7791 - acc: 0.7581 - val_loss: 0.4644 - val_precision: 0.7570 - val_recall: 0.7515 - val_acc: 0.7570\n",
            "Epoch 481/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4589 - precision: 0.7462 - recall: 0.7788 - acc: 0.7584 - val_loss: 0.4603 - val_precision: 0.7399 - val_recall: 0.7944 - val_acc: 0.7595\n",
            "Epoch 482/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4585 - precision: 0.7458 - recall: 0.7794 - acc: 0.7584 - val_loss: 0.4587 - val_precision: 0.7447 - val_recall: 0.7833 - val_acc: 0.7592\n",
            "Epoch 483/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4585 - precision: 0.7459 - recall: 0.7785 - acc: 0.7581 - val_loss: 0.4625 - val_precision: 0.7421 - val_recall: 0.7833 - val_acc: 0.7574\n",
            "Epoch 484/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4591 - precision: 0.7459 - recall: 0.7779 - acc: 0.7580 - val_loss: 0.4619 - val_precision: 0.7411 - val_recall: 0.7793 - val_acc: 0.7554\n",
            "Epoch 485/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4587 - precision: 0.7451 - recall: 0.7784 - acc: 0.7576 - val_loss: 0.4594 - val_precision: 0.7560 - val_recall: 0.7597 - val_acc: 0.7591\n",
            "Epoch 486/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4586 - precision: 0.7468 - recall: 0.7779 - acc: 0.7586 - val_loss: 0.4597 - val_precision: 0.7391 - val_recall: 0.7932 - val_acc: 0.7585\n",
            "Epoch 487/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4581 - precision: 0.7456 - recall: 0.7800 - acc: 0.7584 - val_loss: 0.4587 - val_precision: 0.7484 - val_recall: 0.7743 - val_acc: 0.7588\n",
            "Epoch 488/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4589 - precision: 0.7471 - recall: 0.7772 - acc: 0.7586 - val_loss: 0.4634 - val_precision: 0.7377 - val_recall: 0.7901 - val_acc: 0.7564\n",
            "Epoch 489/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4588 - precision: 0.7458 - recall: 0.7785 - acc: 0.7581 - val_loss: 0.4621 - val_precision: 0.7338 - val_recall: 0.7975 - val_acc: 0.7560\n",
            "Epoch 490/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4584 - precision: 0.7470 - recall: 0.7790 - acc: 0.7591 - val_loss: 0.4594 - val_precision: 0.7327 - val_recall: 0.8078 - val_acc: 0.7584\n",
            "Epoch 491/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4584 - precision: 0.7454 - recall: 0.7794 - acc: 0.7581 - val_loss: 0.4588 - val_precision: 0.7677 - val_recall: 0.7241 - val_acc: 0.7544\n",
            "Epoch 492/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4586 - precision: 0.7453 - recall: 0.7793 - acc: 0.7580 - val_loss: 0.4617 - val_precision: 0.7581 - val_recall: 0.7541 - val_acc: 0.7586\n",
            "Epoch 493/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4587 - precision: 0.7465 - recall: 0.7786 - acc: 0.7586 - val_loss: 0.4595 - val_precision: 0.7417 - val_recall: 0.7932 - val_acc: 0.7603\n",
            "Epoch 494/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4581 - precision: 0.7462 - recall: 0.7805 - acc: 0.7590 - val_loss: 0.4617 - val_precision: 0.7409 - val_recall: 0.7823 - val_acc: 0.7562\n",
            "Epoch 495/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4587 - precision: 0.7464 - recall: 0.7781 - acc: 0.7583 - val_loss: 0.4606 - val_precision: 0.7465 - val_recall: 0.7658 - val_acc: 0.7548\n",
            "Epoch 496/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4586 - precision: 0.7461 - recall: 0.7796 - acc: 0.7586 - val_loss: 0.4601 - val_precision: 0.7296 - val_recall: 0.8129 - val_acc: 0.7577\n",
            "Epoch 497/500\n",
            "283028/283028 [==============================] - 2s 8us/step - loss: 0.4579 - precision: 0.7464 - recall: 0.7777 - acc: 0.7582 - val_loss: 0.4618 - val_precision: 0.7497 - val_recall: 0.7625 - val_acc: 0.7558\n",
            "Epoch 498/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4579 - precision: 0.7466 - recall: 0.7803 - acc: 0.7592 - val_loss: 0.4588 - val_precision: 0.7411 - val_recall: 0.7911 - val_acc: 0.7593\n",
            "Epoch 499/500\n",
            "283028/283028 [==============================] - 2s 9us/step - loss: 0.4579 - precision: 0.7465 - recall: 0.7786 - acc: 0.7586 - val_loss: 0.4612 - val_precision: 0.7376 - val_recall: 0.7851 - val_acc: 0.7548\n",
            "Epoch 500/500\n",
            "283028/283028 [==============================] - 3s 9us/step - loss: 0.4585 - precision: 0.7455 - recall: 0.7795 - acc: 0.7582 - val_loss: 0.4611 - val_precision: 0.7547 - val_recall: 0.7554 - val_acc: 0.7568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V5wE7xhpf5Je",
        "colab_type": "code",
        "outputId": "fa583627-d430-48dd-95ce-e76a543cc4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_features, test_targets)\n",
        "print(\"\\nAccuracy: \", score[-1])\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88447/88447 [==============================] - 10s 116us/step\n",
            "\n",
            "Accuracy:  0.7597883478160339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_Jm3MJHlf5Jr",
        "colab_type": "code",
        "outputId": "676acae5-4290-4f38-85ab-f348d9094d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_precision', 'val_recall', 'val_acc', 'loss', 'precision', 'recall', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9pw_Vjgff5Jv",
        "colab_type": "code",
        "outputId": "3d6582ab-2cf6-498f-c565-2bfd595a2e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f06c608f748>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEVCAYAAAARjMm4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlbpN/ES0Xs/6APKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f06c0772a20>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bz-K-U7hf5J0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random forest\n",
        "\n",
        "is a powerful method for regression and classification.\n",
        "\n",
        "To make a prediction, the random forest considers the predictions of several trees. It would not, however, be useful to have many identical trees because all these trees would presumably give you the same prediction, this is why the trees in the forest are randomized.\n",
        "\n",
        "So Bagging, in the context of decision trees, means that we draw a number of bootstrap datasets and fit each to a tree.\n",
        "\n",
        "And to avoid overfitting we set it to 5 trees,The maximum depth of the tree 13.\n",
        "\n",
        "The function to measure the quality of a split. “entropy” for the information gain."
      ]
    },
    {
      "metadata": {
        "id": "lmB6nqnBf5J1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Integer Encoding\n",
        "\n",
        "Use **LabelEncoder** to transfer our data to integer valuse "
      ]
    },
    {
      "metadata": {
        "id": "9fSGhtHgf5J4",
        "colab_type": "code",
        "outputId": "27672427-a06c-468a-c8ac-78e4e1d903f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "df = black_friday_data.apply(LE.fit_transform)\n",
        "df.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Product_ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>City_Category</th>\n",
              "      <th>Stay_In_Current_City_Years</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Product_Category_1</th>\n",
              "      <th>Product_Category_2</th>\n",
              "      <th>Product_Category_3</th>\n",
              "      <th>Purchase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>666</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2363</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>11764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>818</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>1820</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>1734</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>14715</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   User_ID  Product_ID  Gender  Age  Occupation  City_Category  \\\n",
              "0        0         666       0    0          10              0   \n",
              "1        0        2363       0    0          10              0   \n",
              "3        0         818       0    0          10              0   \n",
              "5        2        1820       1    2          15              0   \n",
              "6        3        1734       1    4           7              1   \n",
              "\n",
              "   Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
              "0                           2               0                   2   \n",
              "1                           2               0                   0   \n",
              "3                           2               0                  11   \n",
              "5                           3               0                   0   \n",
              "6                           2               1                   0   \n",
              "\n",
              "   Product_Category_2  Product_Category_3  Purchase  \n",
              "0                   0                   0      6512  \n",
              "1                   5                  11     11764  \n",
              "3                  13                   0       373  \n",
              "5                   1                   0     11791  \n",
              "6                   7                  14     14715  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "jBZdYyCbf5KA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting the data into training, testing, and validation sets\n",
        "\n",
        "We'll save approximately 20% data as test set after we've trained the network. We'll use this set to make predictions and compare them with the actual numbers.\n",
        "\n",
        "Then Separate the data into features and targets."
      ]
    },
    {
      "metadata": {
        "id": "NSyomxomf5KG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df , test_size=0.2)\n",
        "\n",
        "target_fields = ['Marital_Status']\n",
        "\n",
        "train_features, train_targets = train.drop(target_fields, axis=1), train[target_fields]\n",
        "test_features, test_targets = test.drop(target_fields, axis=1), test[target_fields]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1hnV4wEf5KO",
        "colab_type": "code",
        "outputId": "20e41d81-c9f5-4cfd-bcab-1e231c5490d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=5, criterion='entropy', max_depth=12)\n",
        "clf.fit(train_features, train_targets)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
              "            max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
              "            oob_score=False, random_state=None, verbose=0,\n",
              "            warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "6nfvNt2if5KW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see it gives us high Accuracy."
      ]
    },
    {
      "metadata": {
        "id": "5ovXq3VMf5KY",
        "colab_type": "code",
        "outputId": "b048d928-41f2-46c4-b471-b3f5aa13d0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"As you can see it gives us high Accuracy Score of Random Forests on test set\",clf.score(test_features, test_targets)*100)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "As you can see it gives us high Accuracy Score of Random Forests on test set 82.1644600721336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EVhJ8WOTf5Kk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Naive Bayes Classifier\n",
        "\n",
        "The Naive Bayes Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high like in our case. Despite its simplicity, Naive Bayes can often outperform more sophisticated classification methods.\n",
        "\n",
        "\n",
        "To demonstrate the concept of Naïve Bayes Classification.\n",
        "As indicated, the objects can be classified as either **single or married**. Our task is to classify new cases as they arrive, decide to which class label they belong, based on the currently exiting objects.\n",
        "\n",
        "Since there are more as many **single** objects as **married**, it is reasonable to believe that a new case (which hasn't been observed yet) is more as likely to have membership **single** rather than **married**. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of **single or married** objects, and often used to predict outcomes before they actually happen.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UIS3eVw6f5Kn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y2MaE4QAf5Ks",
        "colab_type": "code",
        "outputId": "3f3ce399-ad6f-481f-91d9-7b6a6360dd59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "model = GaussianNB()\n",
        "\n",
        "\n",
        "model.fit(train_features, train_targets)\n",
        "\n",
        "predicted_labels = model.predict(test_features)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "36Q10dXqf5K0",
        "colab_type": "code",
        "outputId": "dc159e9f-684e-48d4-e958-27b99f4c64d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"FINISHED classifying. accuracy score : \",accuracy_score(test_targets, predicted_labels))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHED classifying. accuracy score :  0.6144583762026976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AAby7o7Rf5K6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}